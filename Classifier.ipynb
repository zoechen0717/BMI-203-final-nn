{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Transcription factors are proteins that bind DNA at promoters to drive gene expression. Most preferentially bind to specific sequences while ignoring others. Traditional methods to determine these sequences (called motifs) have assumed that binding sites in the genome are all independent. However, in some cases people have identified motifs where positional interdependencies exist.\n",
    "\n",
    "# Task\n",
    "Implement a multi-layer fully connected neural network using your NeuralNetwork class to predict whether a short DNA sequence is a binding site for the yeast transcription factor Rap1. The training data is incredibly imbalanced, with way fewer positive sequences than negative sequences, so you will implement a sampling scheme to ensure that class imbalance does not affect training. As in step 2, all of the following work should be done in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "from nn.nn import NeuralNetwork\n",
    "from nn.preprocess import sample_seqs, one_hot_encode_seqs\n",
    "# Import the read_text_file and read_fasta_file functions from the nn.io module\n",
    "from nn.io import read_text_file, read_fasta_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "- Use the read_text_file function from io.py to read in the 137 positive Rap1 motif examples.\n",
    "- Use the read_fasta_file function from io.py to read in all the negative examples. Note that these sequences are much longer than the positive sequences, so you will need to process them to the same length.\n",
    "- Balance your classes using your sample_seq function and explain why you chose the sampling scheme you did.\n",
    "- One-hot encode the data using your one_hot_encode_seqs function.\n",
    "- Split the data into training and validation sets.\n",
    "- Generate an instance of your NeuralNetwork class with an appropriate architecture.\n",
    "- Train your neural network on the training data.\n",
    "- Plot your training and validation loss by epoch.\n",
    "- Report the accuracy of your classifier on your validation dataset.\n",
    "- Explain your choice of loss function and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read inRap1 motif examples & the negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the motif samples\n",
    "positive_seqs = read_text_file('data/rap1-lieb-positives.txt')\n",
    "# Read the fasta files\n",
    "negative_seqs = read_fasta_file('data/yeast-upstream-1k-negative.fa')\n",
    "# Balance classes using sample_seq function from io.py\n",
    "seq_length = len(positive_seqs[0])\n",
    "negative_seqs = [seq[:seq_length] for seq in negative_seqs]  # Truncate negative sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Balance your classes & One-hot encode the data\n",
    "\n",
    "# Explain why you chose the sampling scheme you did\n",
    "Since we have limited positive seqs and much more nagative one, this balancing scheme is designed to preserve all positive samples while randomly downsampling negative samples in proportion to the imbalance ratio. Also, because the Rap1 motif sequence of interest, this scheme will preserve as much as information from the motif while process the yeast upsteam sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance classes\n",
    "X, y = sample_seqs(positive_seqs + negative_seqs, [1] * len(positive_seqs) + [0] * len(negative_seqs))\n",
    "X_encoded = one_hot_encode_seqs(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X_encoded[:train_size], X_encoded[train_size:]\n",
    "y_train = np.array(y[:train_size]).reshape(-1, 1)\n",
    "y_val = np.array(y[train_size:]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate a NeuralNetwork architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 68, \"output_dim\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 128, \"output_dim\": 68, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 68, \"output_dim\": 1, \"activation\": \"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train your neural network on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.6230404232697213, Val Loss: 0.885779623661141\n",
      "Epoch: 1, Train Loss: 0.6060864703798626, Val Loss: 0.8879681377393269\n",
      "Epoch: 2, Train Loss: 0.5897404911846956, Val Loss: 0.8835888286015233\n",
      "Epoch: 3, Train Loss: 0.5781072918483886, Val Loss: 0.7872202895875043\n",
      "Epoch: 4, Train Loss: 0.5609549970902606, Val Loss: 0.8212842805717214\n",
      "Epoch: 5, Train Loss: 0.5480520368225228, Val Loss: 0.8489733610313217\n",
      "Epoch: 6, Train Loss: 0.5362686097141218, Val Loss: 0.8085317606540291\n",
      "Epoch: 7, Train Loss: 0.524958388688975, Val Loss: 0.7823915169944322\n",
      "Epoch: 8, Train Loss: 0.5137959536211301, Val Loss: 0.7697152313273379\n",
      "Epoch: 9, Train Loss: 0.5030790016792267, Val Loss: 0.7683399828686132\n",
      "Epoch: 10, Train Loss: 0.49190724884676845, Val Loss: 0.7392368748465257\n",
      "Epoch: 11, Train Loss: 0.4809194593968208, Val Loss: 0.727702192436278\n",
      "Epoch: 12, Train Loss: 0.47005232627850413, Val Loss: 0.7085774523831021\n",
      "Epoch: 13, Train Loss: 0.4593245857791281, Val Loss: 0.6583405010520618\n",
      "Epoch: 14, Train Loss: 0.4483434572902686, Val Loss: 0.6504315903645076\n",
      "Epoch: 15, Train Loss: 0.4376662252162279, Val Loss: 0.6244308939100529\n",
      "Epoch: 16, Train Loss: 0.42725504964629357, Val Loss: 0.6585045784599426\n",
      "Epoch: 17, Train Loss: 0.415626548389584, Val Loss: 0.6056275323908301\n",
      "Epoch: 18, Train Loss: 0.40512827119056743, Val Loss: 0.5654934967293208\n",
      "Epoch: 19, Train Loss: 0.39688914527273067, Val Loss: 0.5093180708771454\n",
      "Epoch: 20, Train Loss: 0.38338121766638233, Val Loss: 0.5303226196638543\n",
      "Epoch: 21, Train Loss: 0.37202157743662767, Val Loss: 0.5350821663358345\n",
      "Epoch: 22, Train Loss: 0.361401340059546, Val Loss: 0.5245768408219281\n",
      "Epoch: 23, Train Loss: 0.3511373901236997, Val Loss: 0.5294581057588431\n",
      "Epoch: 24, Train Loss: 0.34068974376868266, Val Loss: 0.4779164754245802\n",
      "Epoch: 25, Train Loss: 0.33008558432002777, Val Loss: 0.46969519661576853\n",
      "Epoch: 26, Train Loss: 0.31975866700434485, Val Loss: 0.4639917521997782\n",
      "Epoch: 27, Train Loss: 0.30967045256856524, Val Loss: 0.44838816531856746\n",
      "Epoch: 28, Train Loss: 0.3000848335124449, Val Loss: 0.44819560905738937\n",
      "Epoch: 29, Train Loss: 0.2905464893358221, Val Loss: 0.4242866310399193\n",
      "Epoch: 30, Train Loss: 0.28167393243321864, Val Loss: 0.4243738512835476\n",
      "Epoch: 31, Train Loss: 0.27292802143720024, Val Loss: 0.3782515753929006\n",
      "Epoch: 32, Train Loss: 0.26423384523117127, Val Loss: 0.3951110207606435\n",
      "Epoch: 33, Train Loss: 0.2562291741474712, Val Loss: 0.39109339687390743\n",
      "Epoch: 34, Train Loss: 0.24790022131957515, Val Loss: 0.3618757984439835\n",
      "Epoch: 35, Train Loss: 0.24034623020622928, Val Loss: 0.34020826699739987\n",
      "Epoch: 36, Train Loss: 0.23307721353063585, Val Loss: 0.3248484800689267\n",
      "Epoch: 37, Train Loss: 0.2258321543166534, Val Loss: 0.31904024631195377\n",
      "Epoch: 38, Train Loss: 0.21883205711118192, Val Loss: 0.3124784411638585\n",
      "Epoch: 39, Train Loss: 0.21216529017136032, Val Loss: 0.301664149717526\n",
      "Epoch: 40, Train Loss: 0.20567641292886807, Val Loss: 0.2943373538516045\n",
      "Epoch: 41, Train Loss: 0.19970389882255485, Val Loss: 0.27582510337598953\n",
      "Epoch: 42, Train Loss: 0.1935076556989191, Val Loss: 0.27086803208733184\n",
      "Epoch: 43, Train Loss: 0.187480694300427, Val Loss: 0.2720417975408275\n",
      "Epoch: 44, Train Loss: 0.18202241120316845, Val Loss: 0.2761802363068752\n",
      "Epoch: 45, Train Loss: 0.17642675940457528, Val Loss: 0.2559304614462633\n",
      "Epoch: 46, Train Loss: 0.17122633391823508, Val Loss: 0.2454375201665511\n",
      "Epoch: 47, Train Loss: 0.16619631008778268, Val Loss: 0.2383540955619296\n",
      "Epoch: 48, Train Loss: 0.16133868353302716, Val Loss: 0.24128139110044075\n",
      "Epoch: 49, Train Loss: 0.15678657717553993, Val Loss: 0.2399878788037543\n",
      "Epoch: 50, Train Loss: 0.1522544221272756, Val Loss: 0.2322894645732429\n",
      "Epoch: 51, Train Loss: 0.14788594736541938, Val Loss: 0.2245164843297467\n",
      "Epoch: 52, Train Loss: 0.14365946125793602, Val Loss: 0.20753119473395862\n",
      "Epoch: 53, Train Loss: 0.13967250400348544, Val Loss: 0.20012891944068972\n",
      "Epoch: 54, Train Loss: 0.1357195014037991, Val Loss: 0.19831263732559065\n",
      "Epoch: 55, Train Loss: 0.1319711762870954, Val Loss: 0.19783218443463507\n",
      "Epoch: 56, Train Loss: 0.12836086423611195, Val Loss: 0.18963593563324077\n",
      "Epoch: 57, Train Loss: 0.1248831763330063, Val Loss: 0.18600261476356875\n",
      "Epoch: 58, Train Loss: 0.12182418463192066, Val Loss: 0.1712579998188527\n",
      "Epoch: 59, Train Loss: 0.11882570988444743, Val Loss: 0.16358840375513398\n",
      "Epoch: 60, Train Loss: 0.11530047893723863, Val Loss: 0.16836399113911232\n",
      "Epoch: 61, Train Loss: 0.11226323383919969, Val Loss: 0.16843010041468132\n",
      "Epoch: 62, Train Loss: 0.10937928719079544, Val Loss: 0.16700646493318136\n",
      "Epoch: 63, Train Loss: 0.1066497266603815, Val Loss: 0.15713425407414403\n",
      "Epoch: 64, Train Loss: 0.1039097756516916, Val Loss: 0.15674288352716803\n",
      "Epoch: 65, Train Loss: 0.10137181777511198, Val Loss: 0.1586228118388689\n",
      "Epoch: 66, Train Loss: 0.0988481411445653, Val Loss: 0.1543359503492853\n",
      "Epoch: 67, Train Loss: 0.09640141034083083, Val Loss: 0.1482813561776943\n",
      "Epoch: 68, Train Loss: 0.09410222314089343, Val Loss: 0.14235358912385568\n",
      "Epoch: 69, Train Loss: 0.0918589822715439, Val Loss: 0.14010795785716898\n",
      "Epoch: 70, Train Loss: 0.08985014612508568, Val Loss: 0.1314939141288026\n",
      "Epoch: 71, Train Loss: 0.08760549642437272, Val Loss: 0.1333102462540571\n",
      "Epoch: 72, Train Loss: 0.08557216453120729, Val Loss: 0.13498493850830973\n",
      "Epoch: 73, Train Loss: 0.08360822763071048, Val Loss: 0.13059263856824463\n",
      "Epoch: 74, Train Loss: 0.0817218005963898, Val Loss: 0.1274328305145933\n",
      "Epoch: 75, Train Loss: 0.07989836225719622, Val Loss: 0.12546053597789317\n",
      "Epoch: 76, Train Loss: 0.0781659465049151, Val Loss: 0.12041087452434934\n",
      "Epoch: 77, Train Loss: 0.07642365810541063, Val Loss: 0.12189870340952991\n",
      "Epoch: 78, Train Loss: 0.07477564824441421, Val Loss: 0.11916642081271321\n",
      "Epoch: 79, Train Loss: 0.0732026931721927, Val Loss: 0.1155905989025398\n",
      "Epoch: 80, Train Loss: 0.07167434859457576, Val Loss: 0.11228367711586887\n",
      "Epoch: 81, Train Loss: 0.07015863723169471, Val Loss: 0.11177263279256554\n",
      "Epoch: 82, Train Loss: 0.06873373930205481, Val Loss: 0.10805970338835573\n",
      "Epoch: 83, Train Loss: 0.06731090920329139, Val Loss: 0.11083148024932549\n",
      "Epoch: 84, Train Loss: 0.06593992552955329, Val Loss: 0.10705982814961086\n",
      "Epoch: 85, Train Loss: 0.06465743377004257, Val Loss: 0.10243316400563995\n",
      "Epoch: 86, Train Loss: 0.06345239748335142, Val Loss: 0.09879314516449145\n",
      "Epoch: 87, Train Loss: 0.06216769095170682, Val Loss: 0.09854572654147971\n",
      "Epoch: 88, Train Loss: 0.060913235667943336, Val Loss: 0.10061934183384245\n",
      "Epoch: 89, Train Loss: 0.059760698653104516, Val Loss: 0.10081235484299396\n",
      "Epoch: 90, Train Loss: 0.058617460325810944, Val Loss: 0.0982940005015139\n",
      "Epoch: 91, Train Loss: 0.05753448041693354, Val Loss: 0.09443676557395066\n",
      "Epoch: 92, Train Loss: 0.0564564357302757, Val Loss: 0.09482952156384036\n",
      "Epoch: 93, Train Loss: 0.05542416571854192, Val Loss: 0.0942317709017193\n",
      "Epoch: 94, Train Loss: 0.05450428331100785, Val Loss: 0.08721515271283217\n",
      "Epoch: 95, Train Loss: 0.053443845111141045, Val Loss: 0.08905030021524095\n",
      "Epoch: 96, Train Loss: 0.05248746030557282, Val Loss: 0.08992604337205808\n",
      "Epoch: 97, Train Loss: 0.05156605504747516, Val Loss: 0.0886594372589311\n",
      "Epoch: 98, Train Loss: 0.05066989998434494, Val Loss: 0.0854351042176269\n",
      "Epoch: 99, Train Loss: 0.04978378655957107, Val Loss: 0.08529872734010246\n",
      "Epoch: 100, Train Loss: 0.04893761574855124, Val Loss: 0.08579275992949939\n",
      "Epoch: 101, Train Loss: 0.04810196316895134, Val Loss: 0.08267585461106071\n",
      "Epoch: 102, Train Loss: 0.047295249308515266, Val Loss: 0.08131702048470738\n",
      "Epoch: 103, Train Loss: 0.04651751876972878, Val Loss: 0.07969476528253033\n",
      "Epoch: 104, Train Loss: 0.0457649927385164, Val Loss: 0.07794540862468646\n",
      "Epoch: 105, Train Loss: 0.044995012916991775, Val Loss: 0.07898026831084923\n",
      "Epoch: 106, Train Loss: 0.044275416291545466, Val Loss: 0.07708205478036018\n",
      "Epoch: 107, Train Loss: 0.04357223125424554, Val Loss: 0.07763256887461684\n",
      "Epoch: 108, Train Loss: 0.04290093181237895, Val Loss: 0.07433251601016859\n",
      "Epoch: 109, Train Loss: 0.04222758634938544, Val Loss: 0.07360435155379252\n",
      "Epoch: 110, Train Loss: 0.04156546412226203, Val Loss: 0.07316061514539136\n",
      "Epoch: 111, Train Loss: 0.040917571914141845, Val Loss: 0.07399354675594755\n",
      "Epoch: 112, Train Loss: 0.040294366822008335, Val Loss: 0.07301779291525128\n",
      "Epoch: 113, Train Loss: 0.03968782502172656, Val Loss: 0.07158908323938333\n",
      "Epoch: 114, Train Loss: 0.03909710408318478, Val Loss: 0.07130272056953982\n",
      "Epoch: 115, Train Loss: 0.038518776997840225, Val Loss: 0.07038669644005349\n",
      "Epoch: 116, Train Loss: 0.03795505276618905, Val Loss: 0.07054580238315174\n",
      "Epoch: 117, Train Loss: 0.03740088338749742, Val Loss: 0.06953549419876981\n",
      "Epoch: 118, Train Loss: 0.03685879546314772, Val Loss: 0.06812619034255003\n",
      "Epoch: 119, Train Loss: 0.03633827109707826, Val Loss: 0.06690884453768281\n",
      "Epoch: 120, Train Loss: 0.035819559525014914, Val Loss: 0.06646418257617165\n",
      "Epoch: 121, Train Loss: 0.03531385475986255, Val Loss: 0.06603755607467508\n",
      "Epoch: 122, Train Loss: 0.03482031389837551, Val Loss: 0.06532030372414735\n",
      "Epoch: 123, Train Loss: 0.03433894841729889, Val Loss: 0.06523688695764694\n",
      "Epoch: 124, Train Loss: 0.033868994804298544, Val Loss: 0.06447199983550334\n",
      "Epoch: 125, Train Loss: 0.03341382153042898, Val Loss: 0.06286400640909019\n",
      "Epoch: 126, Train Loss: 0.03297025289177767, Val Loss: 0.061542134964543985\n",
      "Epoch: 127, Train Loss: 0.03252708525885837, Val Loss: 0.06105653997368886\n",
      "Epoch: 128, Train Loss: 0.03208777531059575, Val Loss: 0.061670412488248026\n",
      "Epoch: 129, Train Loss: 0.03166527832566611, Val Loss: 0.06116842891591625\n",
      "Epoch: 130, Train Loss: 0.03125385829613199, Val Loss: 0.05999012655742997\n",
      "Epoch: 131, Train Loss: 0.0308490995550401, Val Loss: 0.060136444788279154\n",
      "Epoch: 132, Train Loss: 0.03045225346005158, Val Loss: 0.059492933768499914\n",
      "Epoch: 133, Train Loss: 0.03006375360843069, Val Loss: 0.059098513286055025\n",
      "Epoch: 134, Train Loss: 0.029683960973837083, Val Loss: 0.05833169265919256\n",
      "Epoch: 135, Train Loss: 0.02932214971985592, Val Loss: 0.05876509414641404\n",
      "Epoch: 136, Train Loss: 0.02895523157314402, Val Loss: 0.05715479351410638\n",
      "Epoch: 137, Train Loss: 0.02860197869230439, Val Loss: 0.056210505001194094\n",
      "Epoch: 138, Train Loss: 0.028249541413496594, Val Loss: 0.0561868706382687\n",
      "Epoch: 139, Train Loss: 0.027907325067017975, Val Loss: 0.055772982465927184\n",
      "Epoch: 140, Train Loss: 0.027571359475500507, Val Loss: 0.0554235569356373\n",
      "Epoch: 141, Train Loss: 0.02724262155867266, Val Loss: 0.05482087936946595\n",
      "Epoch: 142, Train Loss: 0.026921302530266932, Val Loss: 0.0545780642310192\n",
      "Epoch: 143, Train Loss: 0.026610271692013805, Val Loss: 0.053307721295229385\n",
      "Epoch: 144, Train Loss: 0.026295015406508842, Val Loss: 0.05363455458720092\n",
      "Epoch: 145, Train Loss: 0.02599062387793366, Val Loss: 0.05357955843094757\n",
      "Epoch: 146, Train Loss: 0.025692123625030455, Val Loss: 0.05288184848753393\n",
      "Epoch: 147, Train Loss: 0.02540240830749197, Val Loss: 0.052579685777889684\n",
      "Epoch: 148, Train Loss: 0.025114518535515835, Val Loss: 0.052282905844467355\n",
      "Epoch: 149, Train Loss: 0.02483225552451854, Val Loss: 0.051577789067747966\n",
      "Epoch: 150, Train Loss: 0.024555107127760444, Val Loss: 0.05154270828323825\n",
      "Epoch: 151, Train Loss: 0.024283557434445567, Val Loss: 0.05120535937958608\n",
      "Epoch: 152, Train Loss: 0.024020051717587115, Val Loss: 0.05002368681205927\n",
      "Epoch: 153, Train Loss: 0.02376178539103354, Val Loss: 0.049457432646441216\n",
      "Epoch: 154, Train Loss: 0.023503863175034258, Val Loss: 0.049163869683469794\n",
      "Epoch: 155, Train Loss: 0.023248096291253587, Val Loss: 0.04933114980663668\n",
      "Epoch: 156, Train Loss: 0.023001202786043944, Val Loss: 0.04964078640396106\n",
      "Epoch: 157, Train Loss: 0.02275673409144067, Val Loss: 0.04906959911883868\n",
      "Epoch: 158, Train Loss: 0.022517067009372765, Val Loss: 0.0483568552554944\n",
      "Epoch: 159, Train Loss: 0.022283624003654343, Val Loss: 0.047901036677059736\n",
      "Epoch: 160, Train Loss: 0.022052569935270938, Val Loss: 0.047997545776979615\n",
      "Epoch: 161, Train Loss: 0.021826095417125057, Val Loss: 0.047116704255833725\n",
      "Epoch: 162, Train Loss: 0.02160237950720421, Val Loss: 0.046980387020984705\n",
      "Epoch: 163, Train Loss: 0.021388998835261876, Val Loss: 0.04733717055556851\n",
      "Epoch: 164, Train Loss: 0.02117223739254493, Val Loss: 0.04666514516507028\n",
      "Epoch: 165, Train Loss: 0.020960721360496246, Val Loss: 0.04621391199603083\n",
      "Epoch: 166, Train Loss: 0.020752676439439816, Val Loss: 0.04580302817610239\n",
      "Epoch: 167, Train Loss: 0.020548148948276904, Val Loss: 0.04561611886550989\n",
      "Epoch: 168, Train Loss: 0.020347168424421993, Val Loss: 0.04523435150545986\n",
      "Epoch: 169, Train Loss: 0.020150251843036128, Val Loss: 0.04464000298833735\n",
      "Epoch: 170, Train Loss: 0.019957119581020328, Val Loss: 0.04423726475462155\n",
      "Epoch: 171, Train Loss: 0.01976798505741894, Val Loss: 0.04366268104957933\n",
      "Epoch: 172, Train Loss: 0.019574790288321434, Val Loss: 0.04390455506220232\n",
      "Epoch: 173, Train Loss: 0.019388988684106087, Val Loss: 0.04358238780651583\n",
      "Epoch: 174, Train Loss: 0.019205205243783625, Val Loss: 0.04343823353022381\n",
      "Epoch: 175, Train Loss: 0.01902725200344024, Val Loss: 0.043085265719084866\n",
      "Epoch: 176, Train Loss: 0.018849550783107304, Val Loss: 0.04281672132345009\n",
      "Epoch: 177, Train Loss: 0.018674014856815146, Val Loss: 0.042684851820549145\n",
      "Epoch: 178, Train Loss: 0.018501660840279027, Val Loss: 0.04245747908948236\n",
      "Epoch: 179, Train Loss: 0.01833205723783722, Val Loss: 0.042496249648012144\n",
      "Epoch: 180, Train Loss: 0.01816583037809999, Val Loss: 0.04234211541504854\n",
      "Epoch: 181, Train Loss: 0.018002500457423836, Val Loss: 0.04185445352409031\n",
      "Epoch: 182, Train Loss: 0.017840741567025552, Val Loss: 0.04147350775801098\n",
      "Epoch: 183, Train Loss: 0.01768197109579208, Val Loss: 0.041878129036533596\n",
      "Epoch: 184, Train Loss: 0.017524595644407445, Val Loss: 0.041554951823987996\n",
      "Epoch: 185, Train Loss: 0.017371245513755674, Val Loss: 0.0404654801773741\n",
      "Epoch: 186, Train Loss: 0.017219543296282003, Val Loss: 0.04015706848214955\n",
      "Epoch: 187, Train Loss: 0.0170688040213367, Val Loss: 0.0402228869950968\n",
      "Epoch: 188, Train Loss: 0.016920887834318325, Val Loss: 0.04009235008165785\n",
      "Epoch: 189, Train Loss: 0.016774842986064773, Val Loss: 0.04000943892119551\n",
      "Epoch: 190, Train Loss: 0.01663132597369152, Val Loss: 0.039969941953117524\n",
      "Epoch: 191, Train Loss: 0.01649431597077332, Val Loss: 0.04005960233010419\n",
      "Epoch: 192, Train Loss: 0.016354289567458265, Val Loss: 0.039284579413835646\n",
      "Epoch: 193, Train Loss: 0.01621791924060241, Val Loss: 0.03888471456163249\n",
      "Epoch: 194, Train Loss: 0.016082679492978494, Val Loss: 0.038674087277682186\n",
      "Epoch: 195, Train Loss: 0.015949286161138814, Val Loss: 0.0385138429411009\n",
      "Epoch: 196, Train Loss: 0.0158173586403355, Val Loss: 0.038482285568232105\n",
      "Epoch: 197, Train Loss: 0.015689160087530563, Val Loss: 0.03795868750222974\n",
      "Epoch: 198, Train Loss: 0.015560464685264262, Val Loss: 0.038105628997498135\n",
      "Epoch: 199, Train Loss: 0.015435147977373009, Val Loss: 0.037662069113554415\n",
      "Epoch: 200, Train Loss: 0.015310689022567055, Val Loss: 0.037444035906625185\n",
      "Epoch: 201, Train Loss: 0.015187248003438132, Val Loss: 0.03734921389791726\n",
      "Epoch: 202, Train Loss: 0.015065646922022093, Val Loss: 0.03723430225796521\n",
      "Epoch: 203, Train Loss: 0.014946716043173986, Val Loss: 0.03724153140152026\n",
      "Epoch: 204, Train Loss: 0.014828607132440661, Val Loss: 0.036936137025801244\n",
      "Epoch: 205, Train Loss: 0.0147116529638518, Val Loss: 0.03703184775088204\n",
      "Epoch: 206, Train Loss: 0.014596446303042535, Val Loss: 0.03678186062958926\n",
      "Epoch: 207, Train Loss: 0.014482919706180337, Val Loss: 0.0366851002570827\n",
      "Epoch: 208, Train Loss: 0.01437114854408802, Val Loss: 0.03646726670177004\n",
      "Epoch: 209, Train Loss: 0.014261212604381932, Val Loss: 0.03610947180450666\n",
      "Epoch: 210, Train Loss: 0.014152530415725124, Val Loss: 0.03571602388803475\n",
      "Epoch: 211, Train Loss: 0.01404379348960895, Val Loss: 0.0358839431273969\n",
      "Epoch: 212, Train Loss: 0.013937392016435388, Val Loss: 0.03584495860894758\n",
      "Epoch: 213, Train Loss: 0.013832419459384218, Val Loss: 0.035376524477386206\n",
      "Epoch: 214, Train Loss: 0.013728472468090168, Val Loss: 0.035303339088818074\n",
      "Epoch: 215, Train Loss: 0.013626667666650326, Val Loss: 0.035246305611110966\n",
      "Epoch: 216, Train Loss: 0.01352585834393249, Val Loss: 0.03498782852649838\n",
      "Epoch: 217, Train Loss: 0.013425955556659798, Val Loss: 0.03496931614846866\n",
      "Epoch: 218, Train Loss: 0.013327509376378052, Val Loss: 0.03465398629342267\n",
      "Epoch: 219, Train Loss: 0.013233022227274648, Val Loss: 0.034808798083153035\n",
      "Epoch: 220, Train Loss: 0.01313718766652357, Val Loss: 0.03428556304779412\n",
      "Epoch: 221, Train Loss: 0.013042485177904219, Val Loss: 0.03415857345263648\n",
      "Epoch: 222, Train Loss: 0.012948753872518827, Val Loss: 0.034191013595377115\n",
      "Epoch: 223, Train Loss: 0.012856428162784752, Val Loss: 0.034016968181892836\n",
      "Epoch: 224, Train Loss: 0.01276518883315401, Val Loss: 0.03381521439908952\n",
      "Epoch: 225, Train Loss: 0.012675237055385952, Val Loss: 0.033547742173967575\n",
      "Epoch: 226, Train Loss: 0.012586439473583748, Val Loss: 0.03364560680193932\n",
      "Epoch: 227, Train Loss: 0.012498344682099155, Val Loss: 0.03340035286385356\n",
      "Epoch: 228, Train Loss: 0.012411280675667715, Val Loss: 0.03322968076300882\n",
      "Epoch: 229, Train Loss: 0.012325191083919529, Val Loss: 0.03324697593324669\n",
      "Epoch: 230, Train Loss: 0.01224005084718274, Val Loss: 0.033119966760484225\n",
      "Epoch: 231, Train Loss: 0.012156791009558797, Val Loss: 0.03303747820019938\n",
      "Epoch: 232, Train Loss: 0.012073287313896348, Val Loss: 0.03282427978808564\n",
      "Epoch: 233, Train Loss: 0.011990895261697448, Val Loss: 0.032677994088333315\n",
      "Epoch: 234, Train Loss: 0.011909499100166109, Val Loss: 0.032465206283334165\n",
      "Epoch: 235, Train Loss: 0.011829037719048679, Val Loss: 0.03239479139303357\n",
      "Epoch: 236, Train Loss: 0.011749843621337085, Val Loss: 0.03217499765458977\n",
      "Epoch: 237, Train Loss: 0.011671735200975064, Val Loss: 0.032042268877610025\n",
      "Epoch: 238, Train Loss: 0.011594448127482141, Val Loss: 0.031727931827849824\n",
      "Epoch: 239, Train Loss: 0.011517118356348326, Val Loss: 0.031785618008309854\n",
      "Epoch: 240, Train Loss: 0.011441211339463938, Val Loss: 0.031698612748359174\n",
      "Epoch: 241, Train Loss: 0.011366430370507967, Val Loss: 0.031414293640142345\n",
      "Epoch: 242, Train Loss: 0.011292205467064161, Val Loss: 0.031308518782748575\n",
      "Epoch: 243, Train Loss: 0.011219495476824904, Val Loss: 0.031142124752575502\n",
      "Epoch: 244, Train Loss: 0.011146790852070779, Val Loss: 0.031164085656788586\n",
      "Epoch: 245, Train Loss: 0.011075098857360322, Val Loss: 0.030985444668768396\n",
      "Epoch: 246, Train Loss: 0.011003991120896174, Val Loss: 0.03097021076359407\n",
      "Epoch: 247, Train Loss: 0.010936777973601345, Val Loss: 0.03125252492531222\n",
      "Epoch: 248, Train Loss: 0.010866787008206499, Val Loss: 0.03096040839651537\n",
      "Epoch: 249, Train Loss: 0.01079817896326347, Val Loss: 0.03079612432279421\n",
      "Epoch: 250, Train Loss: 0.010730495236992625, Val Loss: 0.03072911242463002\n",
      "Epoch: 251, Train Loss: 0.010663447458052311, Val Loss: 0.030584174689171\n",
      "Epoch: 252, Train Loss: 0.010597104546315711, Val Loss: 0.0302864081191815\n",
      "Epoch: 253, Train Loss: 0.01053167966593261, Val Loss: 0.030056636945774925\n",
      "Epoch: 254, Train Loss: 0.0104671405062007, Val Loss: 0.029957358493603262\n",
      "Epoch: 255, Train Loss: 0.010402848528838516, Val Loss: 0.029834274554482773\n",
      "Epoch: 256, Train Loss: 0.01033913172673061, Val Loss: 0.029760786625940117\n",
      "Epoch: 257, Train Loss: 0.010276058139143698, Val Loss: 0.0296986949330239\n",
      "Epoch: 258, Train Loss: 0.010213842662115516, Val Loss: 0.029530525159989378\n",
      "Epoch: 259, Train Loss: 0.010152840159472475, Val Loss: 0.02942900311698694\n",
      "Epoch: 260, Train Loss: 0.010091761117410673, Val Loss: 0.029269961729472188\n",
      "Epoch: 261, Train Loss: 0.010031285879514691, Val Loss: 0.02913812904073386\n",
      "Epoch: 262, Train Loss: 0.00997122661423122, Val Loss: 0.029086629415378196\n",
      "Epoch: 263, Train Loss: 0.009911813299853205, Val Loss: 0.02908780773529503\n",
      "Epoch: 264, Train Loss: 0.0098533987342811, Val Loss: 0.028948333987803405\n",
      "Epoch: 265, Train Loss: 0.00979559414956803, Val Loss: 0.028950539680980453\n",
      "Epoch: 266, Train Loss: 0.009738073601877803, Val Loss: 0.02880150482345316\n",
      "Epoch: 267, Train Loss: 0.009681057233170337, Val Loss: 0.02871656718945034\n",
      "Epoch: 268, Train Loss: 0.009624674160053727, Val Loss: 0.028624143108568038\n",
      "Epoch: 269, Train Loss: 0.009569033402972599, Val Loss: 0.028403560508054652\n",
      "Epoch: 270, Train Loss: 0.009513690245176357, Val Loss: 0.02840108797468593\n",
      "Epoch: 271, Train Loss: 0.009459369337384041, Val Loss: 0.028341759322026837\n",
      "Epoch: 272, Train Loss: 0.009405342196416063, Val Loss: 0.028224774734384134\n",
      "Epoch: 273, Train Loss: 0.009351793088243923, Val Loss: 0.02806064314462778\n",
      "Epoch: 274, Train Loss: 0.009298620601842039, Val Loss: 0.028077967607719995\n",
      "Epoch: 275, Train Loss: 0.009248240330679027, Val Loss: 0.028258821102172538\n",
      "Epoch: 276, Train Loss: 0.009195920480993188, Val Loss: 0.0277805284163667\n",
      "Epoch: 277, Train Loss: 0.009144797111041133, Val Loss: 0.027595171738676472\n",
      "Epoch: 278, Train Loss: 0.009093608606514736, Val Loss: 0.027640765322714075\n",
      "Epoch: 279, Train Loss: 0.009043286404469357, Val Loss: 0.02753382430310713\n",
      "Epoch: 280, Train Loss: 0.008993490407931043, Val Loss: 0.02741082270190147\n",
      "Epoch: 281, Train Loss: 0.008944085511224229, Val Loss: 0.027337345684796704\n",
      "Epoch: 282, Train Loss: 0.008895344350431945, Val Loss: 0.02735397914056481\n",
      "Epoch: 283, Train Loss: 0.00884702362966959, Val Loss: 0.027150574245882315\n",
      "Epoch: 284, Train Loss: 0.008798897785650546, Val Loss: 0.02714418099204094\n",
      "Epoch: 285, Train Loss: 0.00875125337847251, Val Loss: 0.027062812877201515\n",
      "Epoch: 286, Train Loss: 0.0087042770656701, Val Loss: 0.02688268621086537\n",
      "Epoch: 287, Train Loss: 0.008657987231547185, Val Loss: 0.026846380503932708\n",
      "Epoch: 288, Train Loss: 0.008611681771100574, Val Loss: 0.026728965008016043\n",
      "Epoch: 289, Train Loss: 0.008565568680479255, Val Loss: 0.02675476523334271\n",
      "Epoch: 290, Train Loss: 0.008520126783117846, Val Loss: 0.026600069658457345\n",
      "Epoch: 291, Train Loss: 0.008475084724729347, Val Loss: 0.02650034310148068\n",
      "Epoch: 292, Train Loss: 0.008430606491444424, Val Loss: 0.02641023766125711\n",
      "Epoch: 293, Train Loss: 0.008386496824409671, Val Loss: 0.02642084190799871\n",
      "Epoch: 294, Train Loss: 0.008342581905737051, Val Loss: 0.02635474860928922\n",
      "Epoch: 295, Train Loss: 0.008299099371563078, Val Loss: 0.02633350455548299\n",
      "Epoch: 296, Train Loss: 0.008256137722934648, Val Loss: 0.026357858540360317\n",
      "Epoch: 297, Train Loss: 0.008213388759882752, Val Loss: 0.026226173175320455\n",
      "Epoch: 298, Train Loss: 0.008171092647478895, Val Loss: 0.02614586349824665\n",
      "Epoch: 299, Train Loss: 0.008129487067493116, Val Loss: 0.026020091574248148\n",
      "Epoch: 300, Train Loss: 0.008088058715727249, Val Loss: 0.02589331532252642\n",
      "Epoch: 301, Train Loss: 0.00804693579749422, Val Loss: 0.025766035697200294\n",
      "Epoch: 302, Train Loss: 0.008006081346996438, Val Loss: 0.025750133973650557\n",
      "Epoch: 303, Train Loss: 0.007967180912998324, Val Loss: 0.025873760593212003\n",
      "Epoch: 304, Train Loss: 0.007926955855655544, Val Loss: 0.025648909844049852\n",
      "Epoch: 305, Train Loss: 0.007887368604309076, Val Loss: 0.0255227079591635\n",
      "Epoch: 306, Train Loss: 0.007848065970872752, Val Loss: 0.025466181509715094\n",
      "Epoch: 307, Train Loss: 0.007809123000759355, Val Loss: 0.025401316596846513\n",
      "Epoch: 308, Train Loss: 0.007770595180370907, Val Loss: 0.025231958936544077\n",
      "Epoch: 309, Train Loss: 0.007732435757860048, Val Loss: 0.02508779668881456\n",
      "Epoch: 310, Train Loss: 0.007694415903400525, Val Loss: 0.02512121309873964\n",
      "Epoch: 311, Train Loss: 0.007656669901332917, Val Loss: 0.02505910118635534\n",
      "Epoch: 312, Train Loss: 0.00761934096324129, Val Loss: 0.024949565429532184\n",
      "Epoch: 313, Train Loss: 0.007582115142634265, Val Loss: 0.024999414762701608\n",
      "Epoch: 314, Train Loss: 0.007545322197544927, Val Loss: 0.024910974298986595\n",
      "Epoch: 315, Train Loss: 0.007509235984855684, Val Loss: 0.02487739369305458\n",
      "Epoch: 316, Train Loss: 0.007472991242539563, Val Loss: 0.024826738218875037\n",
      "Epoch: 317, Train Loss: 0.007437209448716418, Val Loss: 0.024866121295704546\n",
      "Epoch: 318, Train Loss: 0.0074014786447723, Val Loss: 0.024725628298876402\n",
      "Epoch: 319, Train Loss: 0.007366163011656539, Val Loss: 0.024635033886990756\n",
      "Epoch: 320, Train Loss: 0.0073313047232195505, Val Loss: 0.02458904396236566\n",
      "Epoch: 321, Train Loss: 0.007296780380153449, Val Loss: 0.02449271692397483\n",
      "Epoch: 322, Train Loss: 0.007262307421534183, Val Loss: 0.024396663155612664\n",
      "Epoch: 323, Train Loss: 0.007228140292850864, Val Loss: 0.02435790557232105\n",
      "Epoch: 324, Train Loss: 0.0071942537211319765, Val Loss: 0.02429967294167656\n",
      "Epoch: 325, Train Loss: 0.007160683861805851, Val Loss: 0.024161201550287625\n",
      "Epoch: 326, Train Loss: 0.007127416064792421, Val Loss: 0.02406187416158235\n",
      "Epoch: 327, Train Loss: 0.007094613313521142, Val Loss: 0.02401420594091456\n",
      "Epoch: 328, Train Loss: 0.007061919801972124, Val Loss: 0.023948339156016678\n",
      "Epoch: 329, Train Loss: 0.007029377958892958, Val Loss: 0.023905124411942123\n",
      "Epoch: 330, Train Loss: 0.006997073226488832, Val Loss: 0.02389093133450415\n",
      "Epoch: 331, Train Loss: 0.006966421490313291, Val Loss: 0.02402529707674912\n",
      "Epoch: 332, Train Loss: 0.00693455008721451, Val Loss: 0.023882907880946415\n",
      "Epoch: 333, Train Loss: 0.006903140758593965, Val Loss: 0.02380846122543406\n",
      "Epoch: 334, Train Loss: 0.0068719185726101835, Val Loss: 0.023681767311620867\n",
      "Epoch: 335, Train Loss: 0.0068410429920663825, Val Loss: 0.023621935530115455\n",
      "Epoch: 336, Train Loss: 0.006810458091969654, Val Loss: 0.023470589562621467\n",
      "Epoch: 337, Train Loss: 0.0067800224139342195, Val Loss: 0.02341108363849934\n",
      "Epoch: 338, Train Loss: 0.006749963232834454, Val Loss: 0.023428619615502393\n",
      "Epoch: 339, Train Loss: 0.0067199571886679115, Val Loss: 0.023341961001216983\n",
      "Epoch: 340, Train Loss: 0.006690219732024306, Val Loss: 0.023259815398731905\n",
      "Epoch: 341, Train Loss: 0.006660665370122208, Val Loss: 0.023233628826796436\n",
      "Epoch: 342, Train Loss: 0.0066313561859127545, Val Loss: 0.023207272852252023\n",
      "Epoch: 343, Train Loss: 0.0066026242599873065, Val Loss: 0.02311368588566729\n",
      "Epoch: 344, Train Loss: 0.006573774538538028, Val Loss: 0.022995979646772566\n",
      "Epoch: 345, Train Loss: 0.006545027847968949, Val Loss: 0.022994072162183167\n",
      "Epoch: 346, Train Loss: 0.006516586781282995, Val Loss: 0.022909248140331136\n",
      "Epoch: 347, Train Loss: 0.006488336045418887, Val Loss: 0.022875413683079632\n",
      "Epoch: 348, Train Loss: 0.006460418247641855, Val Loss: 0.022814907308978078\n",
      "Epoch: 349, Train Loss: 0.006432742729290042, Val Loss: 0.022782371743761864\n",
      "Epoch: 350, Train Loss: 0.006405106914098366, Val Loss: 0.022693608813360237\n",
      "Epoch: 351, Train Loss: 0.006377626362610238, Val Loss: 0.022692614701926624\n",
      "Epoch: 352, Train Loss: 0.006350391086625369, Val Loss: 0.02263420049810883\n",
      "Epoch: 353, Train Loss: 0.006323424848464935, Val Loss: 0.022517580759414207\n",
      "Epoch: 354, Train Loss: 0.0062966063697712776, Val Loss: 0.02245977711445254\n",
      "Epoch: 355, Train Loss: 0.006270142470035722, Val Loss: 0.02243449375145659\n",
      "Epoch: 356, Train Loss: 0.006243730602364508, Val Loss: 0.0224299468936553\n",
      "Epoch: 357, Train Loss: 0.006217507322444239, Val Loss: 0.02235344021964392\n",
      "Epoch: 358, Train Loss: 0.00619143928834037, Val Loss: 0.022299112630266633\n",
      "Epoch: 359, Train Loss: 0.006166549521470752, Val Loss: 0.022396259481023046\n",
      "Epoch: 360, Train Loss: 0.006140846875992594, Val Loss: 0.02228212787253749\n",
      "Epoch: 361, Train Loss: 0.006115423278777678, Val Loss: 0.022248502794750692\n",
      "Epoch: 362, Train Loss: 0.006090166026328017, Val Loss: 0.02219575044857829\n",
      "Epoch: 363, Train Loss: 0.006065109935943764, Val Loss: 0.022127282731597985\n",
      "Epoch: 364, Train Loss: 0.006040214601092049, Val Loss: 0.02201363661532969\n",
      "Epoch: 365, Train Loss: 0.00601547976774914, Val Loss: 0.022000607677860523\n",
      "Epoch: 366, Train Loss: 0.005991070549500002, Val Loss: 0.02196134136563141\n",
      "Epoch: 367, Train Loss: 0.005966751261425051, Val Loss: 0.021787325520728134\n",
      "Epoch: 368, Train Loss: 0.00594259874996918, Val Loss: 0.021702495605683984\n",
      "Epoch: 369, Train Loss: 0.005918423276921531, Val Loss: 0.021713048604567776\n",
      "Epoch: 370, Train Loss: 0.005894549772689888, Val Loss: 0.021652901119149574\n",
      "Epoch: 371, Train Loss: 0.005871061205083143, Val Loss: 0.02163655245264149\n",
      "Epoch: 372, Train Loss: 0.005847454247185518, Val Loss: 0.021587072189278526\n",
      "Epoch: 373, Train Loss: 0.005824012020353573, Val Loss: 0.02154781182318748\n",
      "Epoch: 374, Train Loss: 0.005800776228949555, Val Loss: 0.02147972404570155\n",
      "Epoch: 375, Train Loss: 0.005777695072916438, Val Loss: 0.02140921631673024\n",
      "Epoch: 376, Train Loss: 0.005754815601179278, Val Loss: 0.021364727382769483\n",
      "Epoch: 377, Train Loss: 0.0057321530845107155, Val Loss: 0.02133188588925586\n",
      "Epoch: 378, Train Loss: 0.005709486168454045, Val Loss: 0.021304084442979973\n",
      "Epoch: 379, Train Loss: 0.0056870383829223155, Val Loss: 0.02122021893339841\n",
      "Epoch: 380, Train Loss: 0.005664644038982753, Val Loss: 0.021229950654465996\n",
      "Epoch: 381, Train Loss: 0.0056425086727774415, Val Loss: 0.021131717983940566\n",
      "Epoch: 382, Train Loss: 0.0056205437475810634, Val Loss: 0.02105290596880775\n",
      "Epoch: 383, Train Loss: 0.0055987777951500275, Val Loss: 0.021058171105266123\n",
      "Epoch: 384, Train Loss: 0.0055770622772954494, Val Loss: 0.02106651070248259\n",
      "Epoch: 385, Train Loss: 0.005555470952616179, Val Loss: 0.02101800119685796\n",
      "Epoch: 386, Train Loss: 0.005534014924701916, Val Loss: 0.020981910466179754\n",
      "Epoch: 387, Train Loss: 0.005513751278756116, Val Loss: 0.02111970486285165\n",
      "Epoch: 388, Train Loss: 0.005492482594635715, Val Loss: 0.020999840757441334\n",
      "Epoch: 389, Train Loss: 0.005471493626484181, Val Loss: 0.020918103871443848\n",
      "Epoch: 390, Train Loss: 0.005450664729311644, Val Loss: 0.020850096689759318\n",
      "Epoch: 391, Train Loss: 0.005430001261880186, Val Loss: 0.020753163552006904\n",
      "Epoch: 392, Train Loss: 0.005409506086192656, Val Loss: 0.020654763726618907\n",
      "Epoch: 393, Train Loss: 0.005389118161471823, Val Loss: 0.02058625620394326\n",
      "Epoch: 394, Train Loss: 0.00536891665138047, Val Loss: 0.020576533407807057\n",
      "Epoch: 395, Train Loss: 0.0053487606419434135, Val Loss: 0.020507577861235606\n",
      "Epoch: 396, Train Loss: 0.005328710641784563, Val Loss: 0.020478281195515848\n",
      "Epoch: 397, Train Loss: 0.00530875941541191, Val Loss: 0.020465481279339948\n",
      "Epoch: 398, Train Loss: 0.00528896904552474, Val Loss: 0.020429914451395918\n",
      "Epoch: 399, Train Loss: 0.005269526510092802, Val Loss: 0.020416778821792568\n",
      "Epoch: 400, Train Loss: 0.005249956220357352, Val Loss: 0.020357684252922385\n",
      "Epoch: 401, Train Loss: 0.005230503430089701, Val Loss: 0.020344683911363813\n",
      "Epoch: 402, Train Loss: 0.005211205924687816, Val Loss: 0.020284699370076986\n",
      "Epoch: 403, Train Loss: 0.005192051481611511, Val Loss: 0.020212320091742307\n",
      "Epoch: 404, Train Loss: 0.005173067052647352, Val Loss: 0.020219463413601136\n",
      "Epoch: 405, Train Loss: 0.005154247442225181, Val Loss: 0.020175914511161203\n",
      "Epoch: 406, Train Loss: 0.005135420461762192, Val Loss: 0.020117049420189617\n",
      "Epoch: 407, Train Loss: 0.005116714225673327, Val Loss: 0.020075327618303742\n",
      "Epoch: 408, Train Loss: 0.005098141874008679, Val Loss: 0.02007689731256185\n",
      "Epoch: 409, Train Loss: 0.005079679616470318, Val Loss: 0.019995709061583797\n",
      "Epoch: 410, Train Loss: 0.005061354310592209, Val Loss: 0.019937714136997953\n",
      "Epoch: 411, Train Loss: 0.005043282453889496, Val Loss: 0.019906064564628886\n",
      "Epoch: 412, Train Loss: 0.005025219138174389, Val Loss: 0.019883296833443944\n",
      "Epoch: 413, Train Loss: 0.005007234020340284, Val Loss: 0.01981730126311216\n",
      "Epoch: 414, Train Loss: 0.004989351294279219, Val Loss: 0.01978530191292152\n",
      "Epoch: 415, Train Loss: 0.004972323530067317, Val Loss: 0.019883418557190124\n",
      "Epoch: 416, Train Loss: 0.004954654384955603, Val Loss: 0.019792065466748106\n",
      "Epoch: 417, Train Loss: 0.004937156276081517, Val Loss: 0.01973092418314098\n",
      "Epoch: 418, Train Loss: 0.004919781430358398, Val Loss: 0.01969746634281593\n",
      "Epoch: 419, Train Loss: 0.004902520977932054, Val Loss: 0.01964390462253059\n",
      "Epoch: 420, Train Loss: 0.004885362666659953, Val Loss: 0.019564659568876375\n",
      "Epoch: 421, Train Loss: 0.0048683020558940955, Val Loss: 0.019508932682868872\n",
      "Epoch: 422, Train Loss: 0.004851418047263782, Val Loss: 0.01952042430408846\n",
      "Epoch: 423, Train Loss: 0.0048345716612061785, Val Loss: 0.01947414490170563\n",
      "Epoch: 424, Train Loss: 0.004817851392462961, Val Loss: 0.01942271691836658\n",
      "Epoch: 425, Train Loss: 0.004801207407562967, Val Loss: 0.019371884139962842\n",
      "Epoch: 426, Train Loss: 0.004784665175993418, Val Loss: 0.01934174183624053\n",
      "Epoch: 427, Train Loss: 0.004768409006122701, Val Loss: 0.019321419627477136\n",
      "Epoch: 428, Train Loss: 0.004752040733896835, Val Loss: 0.019305622832304855\n",
      "Epoch: 429, Train Loss: 0.0047357968467448, Val Loss: 0.019300061450038927\n",
      "Epoch: 430, Train Loss: 0.004719644175736454, Val Loss: 0.01926276904530267\n",
      "Epoch: 431, Train Loss: 0.0047035846053781185, Val Loss: 0.019212581023035163\n",
      "Epoch: 432, Train Loss: 0.004687695652996755, Val Loss: 0.019192479665282713\n",
      "Epoch: 433, Train Loss: 0.004671938760161395, Val Loss: 0.01915976295490981\n",
      "Epoch: 434, Train Loss: 0.004656145762099645, Val Loss: 0.019072945972548664\n",
      "Epoch: 435, Train Loss: 0.004640477852433, Val Loss: 0.019030027354711997\n",
      "Epoch: 436, Train Loss: 0.004624940269472766, Val Loss: 0.018988693544627757\n",
      "Epoch: 437, Train Loss: 0.004609532878058256, Val Loss: 0.01892570169607225\n",
      "Epoch: 438, Train Loss: 0.0045941134160061795, Val Loss: 0.01889578541048691\n",
      "Epoch: 439, Train Loss: 0.00457898095680043, Val Loss: 0.018855950062268236\n",
      "Epoch: 440, Train Loss: 0.004563782451417231, Val Loss: 0.018830179010004194\n",
      "Epoch: 441, Train Loss: 0.004548637772351214, Val Loss: 0.018810383639001404\n",
      "Epoch: 442, Train Loss: 0.004533645650764769, Val Loss: 0.018760222402075536\n",
      "Epoch: 443, Train Loss: 0.004519240158485752, Val Loss: 0.018830211247142337\n",
      "Epoch: 444, Train Loss: 0.004504433371034333, Val Loss: 0.018769176712659964\n",
      "Epoch: 445, Train Loss: 0.004489748308260844, Val Loss: 0.018738543896619834\n",
      "Epoch: 446, Train Loss: 0.0044751693771767406, Val Loss: 0.018670402785140986\n",
      "Epoch: 447, Train Loss: 0.004460594018133096, Val Loss: 0.01865850295650766\n",
      "Epoch: 448, Train Loss: 0.004446195505724145, Val Loss: 0.018595060147015317\n",
      "Epoch: 449, Train Loss: 0.00443181297280522, Val Loss: 0.018538134596208705\n",
      "Epoch: 450, Train Loss: 0.004417514456320232, Val Loss: 0.018554127650852313\n",
      "Epoch: 451, Train Loss: 0.004403290918072706, Val Loss: 0.01852473143313921\n",
      "Epoch: 452, Train Loss: 0.00438911984211805, Val Loss: 0.018494964665631223\n",
      "Epoch: 453, Train Loss: 0.004375062024186426, Val Loss: 0.018490543393693937\n",
      "Epoch: 454, Train Loss: 0.0043610372236576565, Val Loss: 0.018438115951237816\n",
      "Epoch: 455, Train Loss: 0.004347270990922902, Val Loss: 0.01840490499421599\n",
      "Epoch: 456, Train Loss: 0.0043334115927614976, Val Loss: 0.018394451472952874\n",
      "Epoch: 457, Train Loss: 0.004319647684768453, Val Loss: 0.01837501172785169\n",
      "Epoch: 458, Train Loss: 0.004305891405366499, Val Loss: 0.01826447737671625\n",
      "Epoch: 459, Train Loss: 0.0042923253157149685, Val Loss: 0.018204369488910275\n",
      "Epoch: 460, Train Loss: 0.0042787643453567935, Val Loss: 0.018195897309188247\n",
      "Epoch: 461, Train Loss: 0.00426538377060067, Val Loss: 0.0181660150717079\n",
      "Epoch: 462, Train Loss: 0.004252003241718576, Val Loss: 0.018125940721889836\n",
      "Epoch: 463, Train Loss: 0.004238606437285032, Val Loss: 0.01812289620378704\n",
      "Epoch: 464, Train Loss: 0.004225355770829995, Val Loss: 0.018095932449127747\n",
      "Epoch: 465, Train Loss: 0.004212161819587355, Val Loss: 0.018052245508004165\n",
      "Epoch: 466, Train Loss: 0.0041990730741226215, Val Loss: 0.01802931921309796\n",
      "Epoch: 467, Train Loss: 0.004186128072894121, Val Loss: 0.01801521531914666\n",
      "Epoch: 468, Train Loss: 0.004173183509606659, Val Loss: 0.017973204525177314\n",
      "Epoch: 469, Train Loss: 0.00416030512945233, Val Loss: 0.017952488192078134\n",
      "Epoch: 470, Train Loss: 0.00414749270017856, Val Loss: 0.01788080016095725\n",
      "Epoch: 471, Train Loss: 0.004135311517774395, Val Loss: 0.01799085707467666\n",
      "Epoch: 472, Train Loss: 0.004122559571696555, Val Loss: 0.01789653766245031\n",
      "Epoch: 473, Train Loss: 0.004109959988462006, Val Loss: 0.017832101023040985\n",
      "Epoch: 474, Train Loss: 0.0040974657994503495, Val Loss: 0.017797248561481488\n",
      "Epoch: 475, Train Loss: 0.004085060531127188, Val Loss: 0.017782630161175938\n",
      "Epoch: 476, Train Loss: 0.004072674259065893, Val Loss: 0.01773705886552607\n",
      "Epoch: 477, Train Loss: 0.004060353901454861, Val Loss: 0.01769014393624001\n",
      "Epoch: 478, Train Loss: 0.0040481652170916305, Val Loss: 0.017689904478963313\n",
      "Epoch: 479, Train Loss: 0.004035959941857995, Val Loss: 0.017669865236975443\n",
      "Epoch: 480, Train Loss: 0.004023837609854016, Val Loss: 0.01761802873489206\n",
      "Epoch: 481, Train Loss: 0.004011786710834501, Val Loss: 0.017583066843770284\n",
      "Epoch: 482, Train Loss: 0.00399981605000535, Val Loss: 0.0175362518351887\n",
      "Epoch: 483, Train Loss: 0.003987996809204509, Val Loss: 0.01752495603248846\n",
      "Epoch: 484, Train Loss: 0.0039760515910829694, Val Loss: 0.017507471105016832\n",
      "Epoch: 485, Train Loss: 0.003964164417642534, Val Loss: 0.01750721023773514\n",
      "Epoch: 486, Train Loss: 0.0039524212570347045, Val Loss: 0.017457704534315888\n",
      "Epoch: 487, Train Loss: 0.003940729621864955, Val Loss: 0.017427136315351047\n",
      "Epoch: 488, Train Loss: 0.003929096216455243, Val Loss: 0.017400906804322384\n",
      "Epoch: 489, Train Loss: 0.003917556051997118, Val Loss: 0.017408867968992086\n",
      "Epoch: 490, Train Loss: 0.0039060442866889226, Val Loss: 0.017367878970678093\n",
      "Epoch: 491, Train Loss: 0.003894568110912629, Val Loss: 0.017332820857152872\n",
      "Epoch: 492, Train Loss: 0.0038831775387482845, Val Loss: 0.017317748340278077\n",
      "Epoch: 493, Train Loss: 0.0038718728371682685, Val Loss: 0.01727054481039681\n",
      "Epoch: 494, Train Loss: 0.0038605770279012963, Val Loss: 0.017240399170414096\n",
      "Epoch: 495, Train Loss: 0.0038494686818264737, Val Loss: 0.01726061718943187\n",
      "Epoch: 496, Train Loss: 0.003838313270942518, Val Loss: 0.017224019827748974\n",
      "Epoch: 497, Train Loss: 0.003827217534723427, Val Loss: 0.017186100280979267\n",
      "Epoch: 498, Train Loss: 0.0038161876408518464, Val Loss: 0.01715550940707575\n",
      "Epoch: 499, Train Loss: 0.0038057587227125285, Val Loss: 0.017250962679645463\n",
      "Epoch: 500, Train Loss: 0.0037947396594874973, Val Loss: 0.017174262540938873\n",
      "Epoch: 501, Train Loss: 0.0037838990101938218, Val Loss: 0.017118175732334116\n",
      "Epoch: 502, Train Loss: 0.003773093722025389, Val Loss: 0.01708700884248458\n",
      "Epoch: 503, Train Loss: 0.0037623964136088133, Val Loss: 0.01703476901204076\n",
      "Epoch: 504, Train Loss: 0.003751763796441434, Val Loss: 0.016971698114659924\n",
      "Epoch: 505, Train Loss: 0.003741163384472859, Val Loss: 0.016930299099934074\n",
      "Epoch: 506, Train Loss: 0.003730626872677553, Val Loss: 0.016916512192007988\n",
      "Epoch: 507, Train Loss: 0.0037201229409053503, Val Loss: 0.016878670501395677\n",
      "Epoch: 508, Train Loss: 0.003709636882353973, Val Loss: 0.016852274853585605\n",
      "Epoch: 509, Train Loss: 0.0036991793174413526, Val Loss: 0.016830079851622867\n",
      "Epoch: 510, Train Loss: 0.003688777359421662, Val Loss: 0.016807248993689676\n",
      "Epoch: 511, Train Loss: 0.0036785403242945054, Val Loss: 0.016815449673012548\n",
      "Epoch: 512, Train Loss: 0.0036682369380966165, Val Loss: 0.01679830376055049\n",
      "Epoch: 513, Train Loss: 0.0036580133959204356, Val Loss: 0.01676767837413531\n",
      "Epoch: 514, Train Loss: 0.003647809874564147, Val Loss: 0.01674643736859947\n",
      "Epoch: 515, Train Loss: 0.0036376887206101376, Val Loss: 0.016716093833801075\n",
      "Epoch: 516, Train Loss: 0.0036276704403989854, Val Loss: 0.016672728352841008\n",
      "Epoch: 517, Train Loss: 0.003617666173703184, Val Loss: 0.01668245861369372\n",
      "Epoch: 518, Train Loss: 0.003607689305595558, Val Loss: 0.016651205336477324\n",
      "Epoch: 519, Train Loss: 0.003597762659417589, Val Loss: 0.016621824801637382\n",
      "Epoch: 520, Train Loss: 0.003587842157036464, Val Loss: 0.016624548254544236\n",
      "Epoch: 521, Train Loss: 0.0035780172884502114, Val Loss: 0.01658344398465516\n",
      "Epoch: 522, Train Loss: 0.00356820261100774, Val Loss: 0.016552680517460304\n",
      "Epoch: 523, Train Loss: 0.0035585516142567085, Val Loss: 0.016536501053468888\n",
      "Epoch: 524, Train Loss: 0.0035488657250028143, Val Loss: 0.016517020070561047\n",
      "Epoch: 525, Train Loss: 0.003539231140945792, Val Loss: 0.016458544061856078\n",
      "Epoch: 526, Train Loss: 0.0035296562273542872, Val Loss: 0.016412226215368902\n",
      "Epoch: 527, Train Loss: 0.003520454401833596, Val Loss: 0.016467034508433728\n",
      "Epoch: 528, Train Loss: 0.003510979053186782, Val Loss: 0.016407738710382754\n",
      "Epoch: 529, Train Loss: 0.0035015213141832563, Val Loss: 0.016380757020129325\n",
      "Epoch: 530, Train Loss: 0.003492111261441705, Val Loss: 0.016348416089320826\n",
      "Epoch: 531, Train Loss: 0.0034827630357617888, Val Loss: 0.016315846039352065\n",
      "Epoch: 532, Train Loss: 0.003473484967801177, Val Loss: 0.016283572989103958\n",
      "Epoch: 533, Train Loss: 0.0034642490737861282, Val Loss: 0.016244236177021136\n",
      "Epoch: 534, Train Loss: 0.0034550416227128546, Val Loss: 0.016248501754563893\n",
      "Epoch: 535, Train Loss: 0.0034458664101339326, Val Loss: 0.016222315326448303\n",
      "Epoch: 536, Train Loss: 0.003436753408475263, Val Loss: 0.01618815429287165\n",
      "Epoch: 537, Train Loss: 0.003427630700387651, Val Loss: 0.016155094895707465\n",
      "Epoch: 538, Train Loss: 0.0034185268517371707, Val Loss: 0.016149287602880272\n",
      "Epoch: 539, Train Loss: 0.003409615918699739, Val Loss: 0.016130021414796237\n",
      "Epoch: 540, Train Loss: 0.0034006008859145676, Val Loss: 0.01610419001047815\n",
      "Epoch: 541, Train Loss: 0.003391629725106625, Val Loss: 0.01608823045844658\n",
      "Epoch: 542, Train Loss: 0.00338272719534096, Val Loss: 0.016056799816447212\n",
      "Epoch: 543, Train Loss: 0.00337385251885061, Val Loss: 0.016049114188505572\n",
      "Epoch: 544, Train Loss: 0.003365047905326373, Val Loss: 0.016029983000353396\n",
      "Epoch: 545, Train Loss: 0.0033563373326358286, Val Loss: 0.016004930439473035\n",
      "Epoch: 546, Train Loss: 0.003347568139186377, Val Loss: 0.01597926468120702\n",
      "Epoch: 547, Train Loss: 0.0033388807583119807, Val Loss: 0.01597331193888629\n",
      "Epoch: 548, Train Loss: 0.003330193835848573, Val Loss: 0.01594520989290784\n",
      "Epoch: 549, Train Loss: 0.0033216009330324942, Val Loss: 0.015874917182604656\n",
      "Epoch: 550, Train Loss: 0.0033130361682594065, Val Loss: 0.015846180209891782\n",
      "Epoch: 551, Train Loss: 0.003304556439760464, Val Loss: 0.015841154230248063\n",
      "Epoch: 552, Train Loss: 0.0032960395350394747, Val Loss: 0.015835403765334464\n",
      "Epoch: 553, Train Loss: 0.003287587862141552, Val Loss: 0.015807544503155198\n",
      "Epoch: 554, Train Loss: 0.0032791302863859114, Val Loss: 0.015792555657797885\n",
      "Epoch: 555, Train Loss: 0.003271069007303972, Val Loss: 0.015839002634023\n",
      "Epoch: 556, Train Loss: 0.003262732801612908, Val Loss: 0.0158051681298328\n",
      "Epoch: 557, Train Loss: 0.0032544375662450797, Val Loss: 0.015770439869421955\n",
      "Epoch: 558, Train Loss: 0.0032461691174843848, Val Loss: 0.015734848280162018\n",
      "Epoch: 559, Train Loss: 0.0032379883846011285, Val Loss: 0.015695221465115875\n",
      "Epoch: 560, Train Loss: 0.003229802115037827, Val Loss: 0.01566876399367373\n",
      "Epoch: 561, Train Loss: 0.003221688688677958, Val Loss: 0.015623788161698364\n",
      "Epoch: 562, Train Loss: 0.003213585207725244, Val Loss: 0.01564264266296997\n",
      "Epoch: 563, Train Loss: 0.0032055156595077855, Val Loss: 0.015604456613625677\n",
      "Epoch: 564, Train Loss: 0.003197510098579856, Val Loss: 0.015562396406963662\n",
      "Epoch: 565, Train Loss: 0.003189451671031816, Val Loss: 0.015554664460430339\n",
      "Epoch: 566, Train Loss: 0.0031814404280972514, Val Loss: 0.015547865463651746\n",
      "Epoch: 567, Train Loss: 0.0031735764831388137, Val Loss: 0.015539644563435689\n",
      "Epoch: 568, Train Loss: 0.0031656519021299005, Val Loss: 0.01552283306963648\n",
      "Epoch: 569, Train Loss: 0.0031577756921013435, Val Loss: 0.015516588707037583\n",
      "Epoch: 570, Train Loss: 0.0031498951947313185, Val Loss: 0.015483926277011928\n",
      "Epoch: 571, Train Loss: 0.003142053834972665, Val Loss: 0.015449147517859418\n",
      "Epoch: 572, Train Loss: 0.003134308476300208, Val Loss: 0.015424720475225875\n",
      "Epoch: 573, Train Loss: 0.003126628902109157, Val Loss: 0.015391059206422846\n",
      "Epoch: 574, Train Loss: 0.0031189264834420617, Val Loss: 0.01535864328631452\n",
      "Epoch: 575, Train Loss: 0.0031112042200134365, Val Loss: 0.015353562450861613\n",
      "Epoch: 576, Train Loss: 0.003103567861378964, Val Loss: 0.015326757554539381\n",
      "Epoch: 577, Train Loss: 0.0030959383813448003, Val Loss: 0.015298185808278577\n",
      "Epoch: 578, Train Loss: 0.00308831793995544, Val Loss: 0.015293549042591434\n",
      "Epoch: 579, Train Loss: 0.0030808361204755935, Val Loss: 0.015295270205369053\n",
      "Epoch: 580, Train Loss: 0.0030733286611421703, Val Loss: 0.015269721038624251\n",
      "Epoch: 581, Train Loss: 0.00306584695954496, Val Loss: 0.015245082254717322\n",
      "Epoch: 582, Train Loss: 0.0030584122902468872, Val Loss: 0.015210413180626773\n",
      "Epoch: 583, Train Loss: 0.0030513116409331305, Val Loss: 0.015278194728824832\n",
      "Epoch: 584, Train Loss: 0.0030439445352263643, Val Loss: 0.015250093973869954\n",
      "Epoch: 585, Train Loss: 0.003036585955573664, Val Loss: 0.015218808201436644\n",
      "Epoch: 586, Train Loss: 0.0030292995455655585, Val Loss: 0.01519955126753763\n",
      "Epoch: 587, Train Loss: 0.003022011053386484, Val Loss: 0.015167422059639214\n",
      "Epoch: 588, Train Loss: 0.003014753886800683, Val Loss: 0.015129583852686443\n",
      "Epoch: 589, Train Loss: 0.00300755998024628, Val Loss: 0.015092014386694205\n",
      "Epoch: 590, Train Loss: 0.003000410919441283, Val Loss: 0.015102143982082664\n",
      "Epoch: 591, Train Loss: 0.002993248503093842, Val Loss: 0.015067948174041283\n",
      "Epoch: 592, Train Loss: 0.002986154841080844, Val Loss: 0.015028565325663065\n",
      "Epoch: 593, Train Loss: 0.0029790321347298813, Val Loss: 0.015013992083332097\n",
      "Epoch: 594, Train Loss: 0.002971989057176855, Val Loss: 0.01497800332630293\n",
      "Epoch: 595, Train Loss: 0.0029650289584331846, Val Loss: 0.01496640578180609\n",
      "Epoch: 596, Train Loss: 0.0029579801544753334, Val Loss: 0.014954684240285886\n",
      "Epoch: 597, Train Loss: 0.0029509955173311624, Val Loss: 0.014936890334263509\n",
      "Epoch: 598, Train Loss: 0.0029440550923987615, Val Loss: 0.014906686790606543\n",
      "Epoch: 599, Train Loss: 0.002937097690253344, Val Loss: 0.01489718023656531\n",
      "Epoch: 600, Train Loss: 0.0029301991565206195, Val Loss: 0.014889662635131815\n",
      "Epoch: 601, Train Loss: 0.002923368141961301, Val Loss: 0.01488929761314788\n",
      "Epoch: 602, Train Loss: 0.0029165155320366623, Val Loss: 0.01486634268179191\n",
      "Epoch: 603, Train Loss: 0.0029097051325686846, Val Loss: 0.014856185200851144\n",
      "Epoch: 604, Train Loss: 0.0029029027213333877, Val Loss: 0.014829033228358742\n",
      "Epoch: 605, Train Loss: 0.0028961535010857417, Val Loss: 0.014810772425204113\n",
      "Epoch: 606, Train Loss: 0.0028894144007128426, Val Loss: 0.014786042431656758\n",
      "Epoch: 607, Train Loss: 0.0028827835217918735, Val Loss: 0.014766227742845096\n",
      "Epoch: 608, Train Loss: 0.002876116156711713, Val Loss: 0.014754262420072176\n",
      "Epoch: 609, Train Loss: 0.0028694798990023702, Val Loss: 0.0147242567028958\n",
      "Epoch: 610, Train Loss: 0.0028628690935745492, Val Loss: 0.014705528482149526\n",
      "Epoch: 611, Train Loss: 0.002856561871228674, Val Loss: 0.01476402601963373\n",
      "Epoch: 612, Train Loss: 0.0028500161530085537, Val Loss: 0.014721767680950097\n",
      "Epoch: 613, Train Loss: 0.002843485342322624, Val Loss: 0.014699240524073192\n",
      "Epoch: 614, Train Loss: 0.002837006442684216, Val Loss: 0.014665361896368152\n",
      "Epoch: 615, Train Loss: 0.002830538517590426, Val Loss: 0.014644042437493789\n",
      "Epoch: 616, Train Loss: 0.0028241338119354494, Val Loss: 0.014595597238657604\n",
      "Epoch: 617, Train Loss: 0.002817741366614656, Val Loss: 0.014565494604261523\n",
      "Epoch: 618, Train Loss: 0.002811374135444211, Val Loss: 0.01455984879585407\n",
      "Epoch: 619, Train Loss: 0.0028049826059179965, Val Loss: 0.014550454025625922\n",
      "Epoch: 620, Train Loss: 0.00279863880239198, Val Loss: 0.014530781294437198\n",
      "Epoch: 621, Train Loss: 0.0027923348195205222, Val Loss: 0.014505150278957955\n",
      "Epoch: 622, Train Loss: 0.0027860702237096635, Val Loss: 0.014472484691352644\n",
      "Epoch: 623, Train Loss: 0.0027798270582000523, Val Loss: 0.014487353870371904\n",
      "Epoch: 624, Train Loss: 0.0027735592914232767, Val Loss: 0.014474252953484219\n",
      "Epoch: 625, Train Loss: 0.0027673426318045283, Val Loss: 0.014449395045556418\n",
      "Epoch: 626, Train Loss: 0.0027611264471620817, Val Loss: 0.014435954823781601\n",
      "Epoch: 627, Train Loss: 0.0027549626772851617, Val Loss: 0.014424576659005395\n",
      "Epoch: 628, Train Loss: 0.002748825198658457, Val Loss: 0.014402600552317174\n",
      "Epoch: 629, Train Loss: 0.002742761222164491, Val Loss: 0.014410332318315386\n",
      "Epoch: 630, Train Loss: 0.002736641605802616, Val Loss: 0.01438376632688934\n",
      "Epoch: 631, Train Loss: 0.002730550902198986, Val Loss: 0.014360634405850893\n",
      "Epoch: 632, Train Loss: 0.0027245027722736623, Val Loss: 0.014342251152617965\n",
      "Epoch: 633, Train Loss: 0.0027184829631941732, Val Loss: 0.014319190266502926\n",
      "Epoch: 634, Train Loss: 0.0027124606076085957, Val Loss: 0.014307357725522425\n",
      "Epoch: 635, Train Loss: 0.002706528115419795, Val Loss: 0.014310469769134061\n",
      "Epoch: 636, Train Loss: 0.002700591898222509, Val Loss: 0.014289373448493271\n",
      "Epoch: 637, Train Loss: 0.0026946370734193636, Val Loss: 0.014257607139274965\n",
      "Epoch: 638, Train Loss: 0.0026887266692149393, Val Loss: 0.014239523299229178\n",
      "Epoch: 639, Train Loss: 0.0026831001492771153, Val Loss: 0.014282759214899852\n",
      "Epoch: 640, Train Loss: 0.002677246948711324, Val Loss: 0.0142189379531467\n",
      "Epoch: 641, Train Loss: 0.0026714447994398262, Val Loss: 0.01418541301140825\n",
      "Epoch: 642, Train Loss: 0.0026656292760114234, Val Loss: 0.014173224690566547\n",
      "Epoch: 643, Train Loss: 0.002659880158125677, Val Loss: 0.014140180187989336\n",
      "Epoch: 644, Train Loss: 0.0026541176615609424, Val Loss: 0.014118687499793604\n",
      "Epoch: 645, Train Loss: 0.002648358052684203, Val Loss: 0.014110136027367697\n",
      "Epoch: 646, Train Loss: 0.0026426571299835206, Val Loss: 0.014112491089060805\n",
      "Epoch: 647, Train Loss: 0.0026369671868384315, Val Loss: 0.014086862363667003\n",
      "Epoch: 648, Train Loss: 0.002631286263783472, Val Loss: 0.014068401502382623\n",
      "Epoch: 649, Train Loss: 0.002625635866012905, Val Loss: 0.014046767211873458\n",
      "Epoch: 650, Train Loss: 0.002619989826948856, Val Loss: 0.014026244197328014\n",
      "Epoch: 651, Train Loss: 0.002614435063738023, Val Loss: 0.014039652883211752\n",
      "Epoch: 652, Train Loss: 0.002608816013394027, Val Loss: 0.014015370834014038\n",
      "Epoch: 653, Train Loss: 0.002603244581017547, Val Loss: 0.014008641590167917\n",
      "Epoch: 654, Train Loss: 0.002597668940111962, Val Loss: 0.013981514047485849\n",
      "Epoch: 655, Train Loss: 0.002592138336385056, Val Loss: 0.013957163671940113\n",
      "Epoch: 656, Train Loss: 0.0025866525732364544, Val Loss: 0.013940807516131078\n",
      "Epoch: 657, Train Loss: 0.0025811733522591294, Val Loss: 0.013945254588801918\n",
      "Epoch: 658, Train Loss: 0.002575704196294894, Val Loss: 0.013924492420233612\n",
      "Epoch: 659, Train Loss: 0.002570256818064886, Val Loss: 0.013904533823997142\n",
      "Epoch: 660, Train Loss: 0.002564803127003985, Val Loss: 0.013899418815145255\n",
      "Epoch: 661, Train Loss: 0.0025594054519496464, Val Loss: 0.013867333571648944\n",
      "Epoch: 662, Train Loss: 0.0025540010984425724, Val Loss: 0.013852502825366675\n",
      "Epoch: 663, Train Loss: 0.002548682801562377, Val Loss: 0.013843216635382466\n",
      "Epoch: 664, Train Loss: 0.002543359744244555, Val Loss: 0.013818429529246501\n",
      "Epoch: 665, Train Loss: 0.002538043002953309, Val Loss: 0.013793474629282787\n",
      "Epoch: 666, Train Loss: 0.0025327083507404756, Val Loss: 0.01378833264781972\n",
      "Epoch: 667, Train Loss: 0.0025276360216675738, Val Loss: 0.013843168676110816\n",
      "Epoch: 668, Train Loss: 0.002522361258387557, Val Loss: 0.01380847055687693\n",
      "Epoch: 669, Train Loss: 0.0025171351116328225, Val Loss: 0.01379038066921115\n",
      "Epoch: 670, Train Loss: 0.002511929074094613, Val Loss: 0.013759911419394807\n",
      "Epoch: 671, Train Loss: 0.0025067344644881125, Val Loss: 0.013735596405609035\n",
      "Epoch: 672, Train Loss: 0.002501550508223068, Val Loss: 0.013715317477052716\n",
      "Epoch: 673, Train Loss: 0.0024964067467565425, Val Loss: 0.013681758017628992\n",
      "Epoch: 674, Train Loss: 0.0024912742991433433, Val Loss: 0.013686042159506391\n",
      "Epoch: 675, Train Loss: 0.0024861304378562083, Val Loss: 0.013674146791947384\n",
      "Epoch: 676, Train Loss: 0.002481044787365405, Val Loss: 0.013644336015493828\n",
      "Epoch: 677, Train Loss: 0.0024759308157130022, Val Loss: 0.013642331941601445\n",
      "Epoch: 678, Train Loss: 0.0024708527080308934, Val Loss: 0.013625784133908088\n",
      "Epoch: 679, Train Loss: 0.0024658500484656836, Val Loss: 0.013614556020094492\n",
      "Epoch: 680, Train Loss: 0.0024608060181195383, Val Loss: 0.013606147558954826\n",
      "Epoch: 681, Train Loss: 0.0024557871654289787, Val Loss: 0.013596612545750859\n",
      "Epoch: 682, Train Loss: 0.0024507619820754623, Val Loss: 0.013571008845096403\n",
      "Epoch: 683, Train Loss: 0.0024457741404999363, Val Loss: 0.013547987403419273\n",
      "Epoch: 684, Train Loss: 0.002440825976598128, Val Loss: 0.013539426235001263\n",
      "Epoch: 685, Train Loss: 0.0024359125430312304, Val Loss: 0.013525699194469592\n",
      "Epoch: 686, Train Loss: 0.0024309621561758483, Val Loss: 0.0135141582291992\n",
      "Epoch: 687, Train Loss: 0.0024260313694776325, Val Loss: 0.013505514758977802\n",
      "Epoch: 688, Train Loss: 0.00242113484119469, Val Loss: 0.01348216952179206\n",
      "Epoch: 689, Train Loss: 0.0024162701702012393, Val Loss: 0.013449640718917473\n",
      "Epoch: 690, Train Loss: 0.002411386567714825, Val Loss: 0.01344690390483842\n",
      "Epoch: 691, Train Loss: 0.0024065855033144363, Val Loss: 0.013441950034873548\n",
      "Epoch: 692, Train Loss: 0.0024017604722837834, Val Loss: 0.01342752156025906\n",
      "Epoch: 693, Train Loss: 0.002396944472085249, Val Loss: 0.013414072888134143\n",
      "Epoch: 694, Train Loss: 0.002392157100802377, Val Loss: 0.013405124501054059\n",
      "Epoch: 695, Train Loss: 0.0023876289160792807, Val Loss: 0.013449913482506546\n",
      "Epoch: 696, Train Loss: 0.0023828521606474025, Val Loss: 0.0134226133518944\n",
      "Epoch: 697, Train Loss: 0.002378106412805342, Val Loss: 0.01339879350901616\n",
      "Epoch: 698, Train Loss: 0.0023733955702822784, Val Loss: 0.013364690729422798\n",
      "Epoch: 699, Train Loss: 0.0023686998639867642, Val Loss: 0.013348855227390283\n",
      "Epoch: 700, Train Loss: 0.0023640340075966374, Val Loss: 0.013323823772192659\n",
      "Epoch: 701, Train Loss: 0.002359361552393998, Val Loss: 0.013307087513802924\n",
      "Epoch: 702, Train Loss: 0.002354750722440036, Val Loss: 0.013315606302282627\n",
      "Epoch: 703, Train Loss: 0.0023501181212695842, Val Loss: 0.01330088986602012\n",
      "Epoch: 704, Train Loss: 0.0023454823760188323, Val Loss: 0.013269773826433968\n",
      "Epoch: 705, Train Loss: 0.0023408756935434623, Val Loss: 0.013250542410163945\n",
      "Epoch: 706, Train Loss: 0.002336296113767284, Val Loss: 0.013231439913541724\n",
      "Epoch: 707, Train Loss: 0.0023317804389077768, Val Loss: 0.013213139484138383\n",
      "Epoch: 708, Train Loss: 0.0023272258695430964, Val Loss: 0.013190151956503837\n",
      "Epoch: 709, Train Loss: 0.002322663348009382, Val Loss: 0.01317978519541258\n",
      "Epoch: 710, Train Loss: 0.0023181447112066217, Val Loss: 0.013155729740992933\n",
      "Epoch: 711, Train Loss: 0.0023136147205485283, Val Loss: 0.013146734530191518\n",
      "Epoch: 712, Train Loss: 0.002309114628515286, Val Loss: 0.013143612273591914\n",
      "Epoch: 713, Train Loss: 0.0023046477145286107, Val Loss: 0.01313902115732497\n",
      "Epoch: 714, Train Loss: 0.0023001837241081176, Val Loss: 0.013123609740380413\n",
      "Epoch: 715, Train Loss: 0.002295716909494098, Val Loss: 0.01311518350875266\n",
      "Epoch: 716, Train Loss: 0.002291287683003581, Val Loss: 0.013094934408150101\n",
      "Epoch: 717, Train Loss: 0.002286854874171806, Val Loss: 0.013079159439811328\n",
      "Epoch: 718, Train Loss: 0.0022824446226446765, Val Loss: 0.013076519393724326\n",
      "Epoch: 719, Train Loss: 0.0022780851211429416, Val Loss: 0.013069173069514065\n",
      "Epoch: 720, Train Loss: 0.0022737298165028765, Val Loss: 0.013060382909225992\n",
      "Epoch: 721, Train Loss: 0.0022693556024542087, Val Loss: 0.013041100504081887\n",
      "Epoch: 722, Train Loss: 0.002265016188410173, Val Loss: 0.01302171694147333\n",
      "Epoch: 723, Train Loss: 0.0022608756310340946, Val Loss: 0.013060990760776284\n",
      "Epoch: 724, Train Loss: 0.0022565719586930964, Val Loss: 0.013030290319699375\n",
      "Epoch: 725, Train Loss: 0.0022522769370900875, Val Loss: 0.0130173312306157\n",
      "Epoch: 726, Train Loss: 0.0022480118901824324, Val Loss: 0.01299351169057098\n",
      "Epoch: 727, Train Loss: 0.002243752343470283, Val Loss: 0.01297206157268195\n",
      "Epoch: 728, Train Loss: 0.00223952802993118, Val Loss: 0.012937875614987208\n",
      "Epoch: 729, Train Loss: 0.0022352842768824, Val Loss: 0.012925856589879733\n",
      "Epoch: 730, Train Loss: 0.002231094009628287, Val Loss: 0.01292276914883876\n",
      "Epoch: 731, Train Loss: 0.0022269027642518166, Val Loss: 0.012889712294830215\n",
      "Epoch: 732, Train Loss: 0.002222733942891169, Val Loss: 0.012863277612012681\n",
      "Epoch: 733, Train Loss: 0.002218528894186095, Val Loss: 0.012862014092654986\n",
      "Epoch: 734, Train Loss: 0.002214348534690708, Val Loss: 0.012853244393378808\n",
      "Epoch: 735, Train Loss: 0.002210215561634466, Val Loss: 0.012861357272315485\n",
      "Epoch: 736, Train Loss: 0.0022060566129269692, Val Loss: 0.012858038179928068\n",
      "Epoch: 737, Train Loss: 0.0022019309961392045, Val Loss: 0.012839364303146299\n",
      "Epoch: 738, Train Loss: 0.002197827707562553, Val Loss: 0.012813188589804268\n",
      "Epoch: 739, Train Loss: 0.0021937137943397913, Val Loss: 0.012805039729792496\n",
      "Epoch: 740, Train Loss: 0.0021896303756921004, Val Loss: 0.012794278350848906\n",
      "Epoch: 741, Train Loss: 0.002185588428455173, Val Loss: 0.012784430961966277\n",
      "Epoch: 742, Train Loss: 0.002181531829254442, Val Loss: 0.012770499624179194\n",
      "Epoch: 743, Train Loss: 0.002177493169288811, Val Loss: 0.012750233297089178\n",
      "Epoch: 744, Train Loss: 0.0021734421612698836, Val Loss: 0.012746341920696362\n",
      "Epoch: 745, Train Loss: 0.0021694334121306457, Val Loss: 0.012721377043354808\n",
      "Epoch: 746, Train Loss: 0.0021654222309731453, Val Loss: 0.012703636875304675\n",
      "Epoch: 747, Train Loss: 0.002161443249431308, Val Loss: 0.012707636506825267\n",
      "Epoch: 748, Train Loss: 0.0021574628892182825, Val Loss: 0.01269789360234095\n",
      "Epoch: 749, Train Loss: 0.0021535048410329886, Val Loss: 0.012679775149710129\n",
      "Epoch: 750, Train Loss: 0.0021495375099854536, Val Loss: 0.012670117502447923\n",
      "Epoch: 751, Train Loss: 0.0021458038684051416, Val Loss: 0.01271946257233299\n",
      "Epoch: 752, Train Loss: 0.002141859962155351, Val Loss: 0.01268858798346092\n",
      "Epoch: 753, Train Loss: 0.002137943738798692, Val Loss: 0.01266983447431183\n",
      "Epoch: 754, Train Loss: 0.0021340570247847493, Val Loss: 0.012651986093829871\n",
      "Epoch: 755, Train Loss: 0.002130184164009367, Val Loss: 0.01262186653993633\n",
      "Epoch: 756, Train Loss: 0.002126339594681728, Val Loss: 0.012589747611645113\n",
      "Epoch: 757, Train Loss: 0.0021224707846636565, Val Loss: 0.012585923540745085\n",
      "Epoch: 758, Train Loss: 0.002118640289856863, Val Loss: 0.012589773664451548\n",
      "Epoch: 759, Train Loss: 0.0021148100396828464, Val Loss: 0.012568956592401608\n",
      "Epoch: 760, Train Loss: 0.0021109849556283384, Val Loss: 0.01255699844539812\n",
      "Epoch: 761, Train Loss: 0.002107175007373024, Val Loss: 0.012549411518004906\n",
      "Epoch: 762, Train Loss: 0.002103384791893583, Val Loss: 0.012540696540852006\n",
      "Epoch: 763, Train Loss: 0.00209965355653093, Val Loss: 0.012546640800350369\n",
      "Epoch: 764, Train Loss: 0.0020958581013242445, Val Loss: 0.012525816629949449\n",
      "Epoch: 765, Train Loss: 0.00209208131129628, Val Loss: 0.012514516450257722\n",
      "Epoch: 766, Train Loss: 0.002088316044649156, Val Loss: 0.01249876696596894\n",
      "Epoch: 767, Train Loss: 0.0020845739729184517, Val Loss: 0.012483335341253352\n",
      "Epoch: 768, Train Loss: 0.00208086157552833, Val Loss: 0.012476468723528047\n",
      "Epoch: 769, Train Loss: 0.00207717356376773, Val Loss: 0.012467616571964692\n",
      "Epoch: 770, Train Loss: 0.002073457896044569, Val Loss: 0.012453436318429446\n",
      "Epoch: 771, Train Loss: 0.0020697685088632094, Val Loss: 0.012437308467756035\n",
      "Epoch: 772, Train Loss: 0.0020660885743878604, Val Loss: 0.012426040027781114\n",
      "Epoch: 773, Train Loss: 0.002062428800649718, Val Loss: 0.012402643514035637\n",
      "Epoch: 774, Train Loss: 0.0020587650800927707, Val Loss: 0.012386534165865817\n",
      "Epoch: 775, Train Loss: 0.002055135978227562, Val Loss: 0.012387940518010223\n",
      "Epoch: 776, Train Loss: 0.002051503376163352, Val Loss: 0.012374395570540648\n",
      "Epoch: 777, Train Loss: 0.002047890375937479, Val Loss: 0.012352554204688792\n",
      "Epoch: 778, Train Loss: 0.0020442644675245127, Val Loss: 0.012344725941850373\n",
      "Epoch: 779, Train Loss: 0.0020408245690222704, Val Loss: 0.012387651515502158\n",
      "Epoch: 780, Train Loss: 0.0020372273824495017, Val Loss: 0.012360779978690024\n",
      "Epoch: 781, Train Loss: 0.0020336655241967165, Val Loss: 0.012349117020142589\n",
      "Epoch: 782, Train Loss: 0.0020301069559180147, Val Loss: 0.012330653959409734\n",
      "Epoch: 783, Train Loss: 0.0020265639194985263, Val Loss: 0.012314274279723423\n",
      "Epoch: 784, Train Loss: 0.0020230359461457853, Val Loss: 0.012296753232933591\n",
      "Epoch: 785, Train Loss: 0.002019518309460933, Val Loss: 0.012289504624543169\n",
      "Epoch: 786, Train Loss: 0.002016027882125284, Val Loss: 0.0122906793064361\n",
      "Epoch: 787, Train Loss: 0.002012506404517203, Val Loss: 0.012272940319103438\n",
      "Epoch: 788, Train Loss: 0.002009020335748113, Val Loss: 0.012254944423987774\n",
      "Epoch: 789, Train Loss: 0.002005541600448951, Val Loss: 0.012235751918751998\n",
      "Epoch: 790, Train Loss: 0.002002057107460657, Val Loss: 0.012227468865754461\n",
      "Epoch: 791, Train Loss: 0.0019986331189255427, Val Loss: 0.012220878038129176\n",
      "Epoch: 792, Train Loss: 0.0019951645253715156, Val Loss: 0.01221331087339321\n",
      "Epoch: 793, Train Loss: 0.0019917116362858285, Val Loss: 0.012206193377239853\n",
      "Epoch: 794, Train Loss: 0.0019882761098409998, Val Loss: 0.012188824740313468\n",
      "Epoch: 795, Train Loss: 0.0019848576185858865, Val Loss: 0.01216832721460504\n",
      "Epoch: 796, Train Loss: 0.001981460242697341, Val Loss: 0.012157024608633155\n",
      "Epoch: 797, Train Loss: 0.0019780679959396524, Val Loss: 0.012156689191095282\n",
      "Epoch: 798, Train Loss: 0.0019746856454208213, Val Loss: 0.012133318606560209\n",
      "Epoch: 799, Train Loss: 0.0019713076016215957, Val Loss: 0.012116835905331956\n",
      "Epoch: 800, Train Loss: 0.0019679215528048528, Val Loss: 0.012109466943251327\n",
      "Epoch: 801, Train Loss: 0.0019645689563357236, Val Loss: 0.012093669629324316\n",
      "Epoch: 802, Train Loss: 0.0019612042026808433, Val Loss: 0.012089904604832776\n",
      "Epoch: 803, Train Loss: 0.0019578806675128136, Val Loss: 0.012090512345721848\n",
      "Epoch: 804, Train Loss: 0.0019545535886154857, Val Loss: 0.012075335306484214\n",
      "Epoch: 805, Train Loss: 0.001951240686147703, Val Loss: 0.012057286239865996\n",
      "Epoch: 806, Train Loss: 0.001947919362100469, Val Loss: 0.012049679122364595\n",
      "Epoch: 807, Train Loss: 0.001944766405890371, Val Loss: 0.012079996697440568\n",
      "Epoch: 808, Train Loss: 0.0019414895887118482, Val Loss: 0.01205689809169483\n",
      "Epoch: 809, Train Loss: 0.0019382133976877948, Val Loss: 0.012045814590019495\n",
      "Epoch: 810, Train Loss: 0.0019349624562360898, Val Loss: 0.012022681325303625\n",
      "Epoch: 811, Train Loss: 0.0019317089176646028, Val Loss: 0.012011985432930411\n",
      "Epoch: 812, Train Loss: 0.001928467010022606, Val Loss: 0.011994649672639376\n",
      "Epoch: 813, Train Loss: 0.0019252468609709372, Val Loss: 0.011970945828122642\n",
      "Epoch: 814, Train Loss: 0.0019220291281428594, Val Loss: 0.011975045128631996\n",
      "Epoch: 815, Train Loss: 0.0019188191029676292, Val Loss: 0.0119584886438409\n",
      "Epoch: 816, Train Loss: 0.0019156248398633417, Val Loss: 0.011940358520099314\n",
      "Epoch: 817, Train Loss: 0.0019124148768543006, Val Loss: 0.011934706047041428\n",
      "Epoch: 818, Train Loss: 0.0019092160563677551, Val Loss: 0.011925731673405897\n",
      "Epoch: 819, Train Loss: 0.001906075540045854, Val Loss: 0.011915693454400524\n",
      "Epoch: 820, Train Loss: 0.0019028858731064623, Val Loss: 0.01191524950598354\n",
      "Epoch: 821, Train Loss: 0.0018997254203786125, Val Loss: 0.011903220685707816\n",
      "Epoch: 822, Train Loss: 0.0018965849884837814, Val Loss: 0.011873506742951652\n",
      "Epoch: 823, Train Loss: 0.00189344952746997, Val Loss: 0.01185313078664442\n",
      "Epoch: 824, Train Loss: 0.001890305516702244, Val Loss: 0.011856998741971329\n",
      "Epoch: 825, Train Loss: 0.0018871886432270137, Val Loss: 0.011856288634131787\n",
      "Epoch: 826, Train Loss: 0.0018840776821479114, Val Loss: 0.011841564537661123\n",
      "Epoch: 827, Train Loss: 0.0018809539092441035, Val Loss: 0.011837813197357052\n",
      "Epoch: 828, Train Loss: 0.001877860995789509, Val Loss: 0.011823721971352836\n",
      "Epoch: 829, Train Loss: 0.0018747800575626393, Val Loss: 0.011806488676013886\n",
      "Epoch: 830, Train Loss: 0.0018716859340527972, Val Loss: 0.011801913785911859\n",
      "Epoch: 831, Train Loss: 0.001868642625155653, Val Loss: 0.011796605049812704\n",
      "Epoch: 832, Train Loss: 0.0018655819583629906, Val Loss: 0.011783456857303705\n",
      "Epoch: 833, Train Loss: 0.001862533852640177, Val Loss: 0.011771975452193082\n",
      "Epoch: 834, Train Loss: 0.0018594957409207408, Val Loss: 0.011756903920353154\n",
      "Epoch: 835, Train Loss: 0.0018565801151122921, Val Loss: 0.011797082282753259\n",
      "Epoch: 836, Train Loss: 0.0018535668050859394, Val Loss: 0.011770694422958992\n",
      "Epoch: 837, Train Loss: 0.0018505637152999748, Val Loss: 0.011749728087382123\n",
      "Epoch: 838, Train Loss: 0.0018475584172328537, Val Loss: 0.01173529233308148\n",
      "Epoch: 839, Train Loss: 0.0018445689310259614, Val Loss: 0.011720825868517203\n",
      "Epoch: 840, Train Loss: 0.001841595611873776, Val Loss: 0.01170237031299567\n",
      "Epoch: 841, Train Loss: 0.0018386174378301253, Val Loss: 0.01168688250980107\n",
      "Epoch: 842, Train Loss: 0.001835654769210968, Val Loss: 0.011693072128313105\n",
      "Epoch: 843, Train Loss: 0.001832698763129062, Val Loss: 0.011677258633530202\n",
      "Epoch: 844, Train Loss: 0.0018297643101050204, Val Loss: 0.011655510187248938\n",
      "Epoch: 845, Train Loss: 0.0018268067180112714, Val Loss: 0.011648719449083472\n",
      "Epoch: 846, Train Loss: 0.0018238625440461435, Val Loss: 0.011638348662839354\n",
      "Epoch: 847, Train Loss: 0.0018209540085262477, Val Loss: 0.011638416576976313\n",
      "Epoch: 848, Train Loss: 0.001818019099422879, Val Loss: 0.011633675790800995\n",
      "Epoch: 849, Train Loss: 0.0018151066633880475, Val Loss: 0.011624037104007869\n",
      "Epoch: 850, Train Loss: 0.001812208381434988, Val Loss: 0.011606536069224646\n",
      "Epoch: 851, Train Loss: 0.0018092988837181275, Val Loss: 0.011600378790649738\n",
      "Epoch: 852, Train Loss: 0.0018064118687133404, Val Loss: 0.011599582534661963\n",
      "Epoch: 853, Train Loss: 0.001803563747806022, Val Loss: 0.011601769158056371\n",
      "Epoch: 854, Train Loss: 0.0018006762029704033, Val Loss: 0.011586409226873115\n",
      "Epoch: 855, Train Loss: 0.0017978060389556734, Val Loss: 0.011572418374107586\n",
      "Epoch: 856, Train Loss: 0.001794953199862939, Val Loss: 0.011561769051716858\n",
      "Epoch: 857, Train Loss: 0.00179209581977287, Val Loss: 0.011550874060778005\n",
      "Epoch: 858, Train Loss: 0.0017892510664440746, Val Loss: 0.01154150108525417\n",
      "Epoch: 859, Train Loss: 0.0017864465577292, Val Loss: 0.011539971766416723\n",
      "Epoch: 860, Train Loss: 0.0017836329400306415, Val Loss: 0.011523955057556434\n",
      "Epoch: 861, Train Loss: 0.0017808083329746557, Val Loss: 0.01151061633513328\n",
      "Epoch: 862, Train Loss: 0.0017780051196612547, Val Loss: 0.01149584528584258\n",
      "Epoch: 863, Train Loss: 0.0017753310542078548, Val Loss: 0.011528771740239445\n",
      "Epoch: 864, Train Loss: 0.0017725486386704179, Val Loss: 0.011505830550451786\n",
      "Epoch: 865, Train Loss: 0.0017697682867618212, Val Loss: 0.011491587576083559\n",
      "Epoch: 866, Train Loss: 0.001767006287904711, Val Loss: 0.011473951731465565\n",
      "Epoch: 867, Train Loss: 0.0017642458045779195, Val Loss: 0.011460544546004607\n",
      "Epoch: 868, Train Loss: 0.0017615037586688666, Val Loss: 0.011441469119890914\n",
      "Epoch: 869, Train Loss: 0.0017587476401055833, Val Loss: 0.011434945024156686\n",
      "Epoch: 870, Train Loss: 0.0017560181124921179, Val Loss: 0.011437443759171405\n",
      "Epoch: 871, Train Loss: 0.0017532874702515435, Val Loss: 0.011418966962496288\n",
      "Epoch: 872, Train Loss: 0.0017505753958799743, Val Loss: 0.011400544538737917\n",
      "Epoch: 873, Train Loss: 0.0017478442909287662, Val Loss: 0.011395810256266783\n",
      "Epoch: 874, Train Loss: 0.0017451389938741206, Val Loss: 0.011382770342599725\n",
      "Epoch: 875, Train Loss: 0.0017424500627723322, Val Loss: 0.01138455388934953\n",
      "Epoch: 876, Train Loss: 0.0017397401854519876, Val Loss: 0.01137994684177436\n",
      "Epoch: 877, Train Loss: 0.0017370403117877467, Val Loss: 0.011371899117236546\n",
      "Epoch: 878, Train Loss: 0.0017343566900915645, Val Loss: 0.011358864611264102\n",
      "Epoch: 879, Train Loss: 0.0017316739526869165, Val Loss: 0.011350242169883423\n",
      "Epoch: 880, Train Loss: 0.001729016764624951, Val Loss: 0.011334893112601468\n",
      "Epoch: 881, Train Loss: 0.001726363643902537, Val Loss: 0.0113372204354331\n",
      "Epoch: 882, Train Loss: 0.0017237115377485082, Val Loss: 0.011319931574116296\n",
      "Epoch: 883, Train Loss: 0.0017210668385780313, Val Loss: 0.011308839972993551\n",
      "Epoch: 884, Train Loss: 0.0017184151945694593, Val Loss: 0.011304592308563512\n",
      "Epoch: 885, Train Loss: 0.001715772878880925, Val Loss: 0.011296503129164933\n",
      "Epoch: 886, Train Loss: 0.001713145440090045, Val Loss: 0.01128443604409859\n",
      "Epoch: 887, Train Loss: 0.001710560529926411, Val Loss: 0.011287058742056391\n",
      "Epoch: 888, Train Loss: 0.0017079582163276785, Val Loss: 0.011277874997163628\n",
      "Epoch: 889, Train Loss: 0.0017053386881564817, Val Loss: 0.011256584086297665\n",
      "Epoch: 890, Train Loss: 0.0017027334125759726, Val Loss: 0.011238673521350944\n",
      "Epoch: 891, Train Loss: 0.0017002739877902185, Val Loss: 0.011271704915379533\n",
      "Epoch: 892, Train Loss: 0.001697683401887991, Val Loss: 0.011253497746179886\n",
      "Epoch: 893, Train Loss: 0.0016951186301074335, Val Loss: 0.011238215874809802\n",
      "Epoch: 894, Train Loss: 0.0016925522622552069, Val Loss: 0.011225937350266989\n",
      "Epoch: 895, Train Loss: 0.001690008694797391, Val Loss: 0.011207231196664197\n",
      "Epoch: 896, Train Loss: 0.0016874565885288857, Val Loss: 0.011197622096901064\n",
      "Epoch: 897, Train Loss: 0.001684909089047707, Val Loss: 0.011189502309381958\n",
      "Epoch: 898, Train Loss: 0.0016823925076318235, Val Loss: 0.011185744749188558\n",
      "Epoch: 899, Train Loss: 0.0016798639863189779, Val Loss: 0.011172959506356878\n",
      "Epoch: 900, Train Loss: 0.0016773389898283256, Val Loss: 0.011162958846263404\n",
      "Epoch: 901, Train Loss: 0.0016748222940278664, Val Loss: 0.011152352664749602\n",
      "Epoch: 902, Train Loss: 0.0016723044572339947, Val Loss: 0.011145787990648694\n",
      "Epoch: 903, Train Loss: 0.0016698254603772186, Val Loss: 0.011142607080016154\n",
      "Epoch: 904, Train Loss: 0.0016673285345273877, Val Loss: 0.011128388470683587\n",
      "Epoch: 905, Train Loss: 0.001664823938916055, Val Loss: 0.011121160621595245\n",
      "Epoch: 906, Train Loss: 0.0016623416802216294, Val Loss: 0.011105405523044135\n",
      "Epoch: 907, Train Loss: 0.0016598525504699917, Val Loss: 0.011099033137250252\n",
      "Epoch: 908, Train Loss: 0.0016573803812104607, Val Loss: 0.011093639658078771\n",
      "Epoch: 909, Train Loss: 0.0016549332392046764, Val Loss: 0.011087962169175222\n",
      "Epoch: 910, Train Loss: 0.0016524777065935314, Val Loss: 0.011071632929323396\n",
      "Epoch: 911, Train Loss: 0.001650012941693426, Val Loss: 0.011065175835744834\n",
      "Epoch: 912, Train Loss: 0.0016475545919852038, Val Loss: 0.011059222742309246\n",
      "Epoch: 913, Train Loss: 0.0016451230433192107, Val Loss: 0.011038303720016582\n",
      "Epoch: 914, Train Loss: 0.001642694420083254, Val Loss: 0.01102697488368652\n",
      "Epoch: 915, Train Loss: 0.0016402742636198512, Val Loss: 0.011031025029367226\n",
      "Epoch: 916, Train Loss: 0.0016378515715843888, Val Loss: 0.011023276022224814\n",
      "Epoch: 917, Train Loss: 0.0016354299164587677, Val Loss: 0.01101437125137851\n",
      "Epoch: 918, Train Loss: 0.0016330150004293573, Val Loss: 0.011009500721723787\n",
      "Epoch: 919, Train Loss: 0.0016307404789306639, Val Loss: 0.011039322612958874\n",
      "Epoch: 920, Train Loss: 0.001628350898545592, Val Loss: 0.011026147742191903\n",
      "Epoch: 921, Train Loss: 0.0016259564227788987, Val Loss: 0.01101028677947364\n",
      "Epoch: 922, Train Loss: 0.0016235750706445552, Val Loss: 0.010993320495449502\n",
      "Epoch: 923, Train Loss: 0.001621214516344299, Val Loss: 0.01097573688315037\n",
      "Epoch: 924, Train Loss: 0.0016188486330068913, Val Loss: 0.010964552151563495\n",
      "Epoch: 925, Train Loss: 0.0016164971564768102, Val Loss: 0.01094746810564241\n",
      "Epoch: 926, Train Loss: 0.0016141504665555242, Val Loss: 0.010951362338232656\n",
      "Epoch: 927, Train Loss: 0.0016118046380419672, Val Loss: 0.010935541468091885\n",
      "Epoch: 928, Train Loss: 0.0016094760135354025, Val Loss: 0.01091553667391496\n",
      "Epoch: 929, Train Loss: 0.00160713227333566, Val Loss: 0.01090706782352628\n",
      "Epoch: 930, Train Loss: 0.001604795171369874, Val Loss: 0.010900953415680412\n",
      "Epoch: 931, Train Loss: 0.0016024809942868598, Val Loss: 0.01090214735409837\n",
      "Epoch: 932, Train Loss: 0.0016001526922968354, Val Loss: 0.010896580266932914\n",
      "Epoch: 933, Train Loss: 0.0015978402644130285, Val Loss: 0.010891593359683387\n",
      "Epoch: 934, Train Loss: 0.0015955241520551507, Val Loss: 0.010880438259506473\n",
      "Epoch: 935, Train Loss: 0.0015932256589006439, Val Loss: 0.01087038400628903\n",
      "Epoch: 936, Train Loss: 0.0015909331435656664, Val Loss: 0.010869187957832382\n",
      "Epoch: 937, Train Loss: 0.0015886596028145308, Val Loss: 0.010863765188972483\n",
      "Epoch: 938, Train Loss: 0.0015863811874965708, Val Loss: 0.01084726967119556\n",
      "Epoch: 939, Train Loss: 0.001584106958906842, Val Loss: 0.010837671938859154\n",
      "Epoch: 940, Train Loss: 0.001581823444318799, Val Loss: 0.010833765521641467\n",
      "Epoch: 941, Train Loss: 0.001579564210431719, Val Loss: 0.010819653745032213\n",
      "Epoch: 942, Train Loss: 0.0015772951227041028, Val Loss: 0.010815906601856145\n",
      "Epoch: 943, Train Loss: 0.001575062686739436, Val Loss: 0.010813221636204583\n",
      "Epoch: 944, Train Loss: 0.0015728163983302747, Val Loss: 0.010804936865586711\n",
      "Epoch: 945, Train Loss: 0.0015705783702262574, Val Loss: 0.010794392727099748\n",
      "Epoch: 946, Train Loss: 0.0015683362237828645, Val Loss: 0.010785048680924426\n",
      "Epoch: 947, Train Loss: 0.0015662009397740735, Val Loss: 0.010812475886067785\n",
      "Epoch: 948, Train Loss: 0.0015639803086926918, Val Loss: 0.01080207336561918\n",
      "Epoch: 949, Train Loss: 0.0015617697128327268, Val Loss: 0.010784613306058741\n",
      "Epoch: 950, Train Loss: 0.0015595578674840974, Val Loss: 0.010775856684782368\n",
      "Epoch: 951, Train Loss: 0.0015573655506553985, Val Loss: 0.010759663799186275\n",
      "Epoch: 952, Train Loss: 0.0015551772663183919, Val Loss: 0.010745957979448946\n",
      "Epoch: 953, Train Loss: 0.0015529821083819805, Val Loss: 0.010734138829613498\n",
      "Epoch: 954, Train Loss: 0.0015508015641767683, Val Loss: 0.010740123235338643\n",
      "Epoch: 955, Train Loss: 0.0015486117923781198, Val Loss: 0.010731623749480246\n",
      "Epoch: 956, Train Loss: 0.0015464462889997958, Val Loss: 0.01071237393725617\n",
      "Epoch: 957, Train Loss: 0.00154427664809036, Val Loss: 0.010701888546754025\n",
      "Epoch: 958, Train Loss: 0.0015421029587737913, Val Loss: 0.010696524102650266\n",
      "Epoch: 959, Train Loss: 0.001539954956399258, Val Loss: 0.010697547236495603\n",
      "Epoch: 960, Train Loss: 0.0015377930210349008, Val Loss: 0.01069196255064365\n",
      "Epoch: 961, Train Loss: 0.0015356345934534547, Val Loss: 0.01068339873296666\n",
      "Epoch: 962, Train Loss: 0.0015334903059724134, Val Loss: 0.010669717209318386\n",
      "Epoch: 963, Train Loss: 0.00153134066205022, Val Loss: 0.010665341889585438\n",
      "Epoch: 964, Train Loss: 0.0015292129282144975, Val Loss: 0.01066428537198135\n",
      "Epoch: 965, Train Loss: 0.0015270919015019067, Val Loss: 0.010659726176326843\n",
      "Epoch: 966, Train Loss: 0.0015249671472877858, Val Loss: 0.01064892992758284\n",
      "Epoch: 967, Train Loss: 0.0015228399396514166, Val Loss: 0.01064452284089401\n",
      "Epoch: 968, Train Loss: 0.0015207278491635677, Val Loss: 0.010639571236180534\n",
      "Epoch: 969, Train Loss: 0.001518608575767792, Val Loss: 0.010627501305198673\n",
      "Epoch: 970, Train Loss: 0.001516507515198989, Val Loss: 0.010622004247363885\n",
      "Epoch: 971, Train Loss: 0.0015144222209307675, Val Loss: 0.010614692604737536\n",
      "Epoch: 972, Train Loss: 0.0015123380281844912, Val Loss: 0.010607014418853825\n",
      "Epoch: 973, Train Loss: 0.001510242942013282, Val Loss: 0.010590825437895657\n",
      "Epoch: 974, Train Loss: 0.0015081607862715623, Val Loss: 0.010580948102975881\n",
      "Epoch: 975, Train Loss: 0.001506190961785219, Val Loss: 0.010611229922920951\n",
      "Epoch: 976, Train Loss: 0.001504112092617887, Val Loss: 0.01059540252705103\n",
      "Epoch: 977, Train Loss: 0.0015020498164759436, Val Loss: 0.010582725988496597\n",
      "Epoch: 978, Train Loss: 0.0014999861002630882, Val Loss: 0.010566696391951628\n",
      "Epoch: 979, Train Loss: 0.00149793649580745, Val Loss: 0.010556606534756156\n",
      "Epoch: 980, Train Loss: 0.0014958945066330788, Val Loss: 0.010539986271296512\n",
      "Epoch: 981, Train Loss: 0.0014938565446440462, Val Loss: 0.010526508775028382\n",
      "Epoch: 982, Train Loss: 0.0014918341646687865, Val Loss: 0.010523811588587742\n",
      "Epoch: 983, Train Loss: 0.0014897919073524372, Val Loss: 0.01051754329759806\n",
      "Epoch: 984, Train Loss: 0.001487762952897787, Val Loss: 0.01050704820066265\n",
      "Epoch: 985, Train Loss: 0.0014857303167663406, Val Loss: 0.01050078807586241\n",
      "Epoch: 986, Train Loss: 0.001483707047633983, Val Loss: 0.010495019705282016\n",
      "Epoch: 987, Train Loss: 0.0014817196831535117, Val Loss: 0.010499835814124257\n",
      "Epoch: 988, Train Loss: 0.0014796976772056198, Val Loss: 0.010489775901406586\n",
      "Epoch: 989, Train Loss: 0.0014776839142406498, Val Loss: 0.010479744953215716\n",
      "Epoch: 990, Train Loss: 0.0014756834205674782, Val Loss: 0.010468422870850775\n",
      "Epoch: 991, Train Loss: 0.0014736793686254005, Val Loss: 0.010462849750470507\n",
      "Epoch: 992, Train Loss: 0.001471696005137137, Val Loss: 0.010454943552217388\n",
      "Epoch: 993, Train Loss: 0.001469715649018897, Val Loss: 0.010455937791671671\n",
      "Epoch: 994, Train Loss: 0.0014677334895477265, Val Loss: 0.010443090785723327\n",
      "Epoch: 995, Train Loss: 0.0014657494111012192, Val Loss: 0.010433909148612695\n",
      "Epoch: 996, Train Loss: 0.0014637771561417988, Val Loss: 0.010422386215067168\n",
      "Epoch: 997, Train Loss: 0.0014618111344357222, Val Loss: 0.010410362787043007\n",
      "Epoch: 998, Train Loss: 0.0014598408242162684, Val Loss: 0.010404998541177395\n",
      "Epoch: 999, Train Loss: 0.0014579008943765256, Val Loss: 0.010402960520659685\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train neural network\n",
    "nn = NeuralNetwork(nn_arch=nn_arch, lr=0.05, seed=42, batch_size=4, epochs=1000, loss_function='binary_cross_entropy')\n",
    "train_loss, val_loss = nn.fit(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot your training and validation loss by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Report the accuracy of your classifier on your validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "y_pred = nn.predict(X_val) > 0.5\n",
    "accuracy = np.mean(y_pred == y_val)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Explain your choice of loss function and hyperparameters.\n",
    "\n",
    "I chose these hyperparameter values based on a balance between training stability and efficiency:\n",
    "\n",
    "- lr=0.05 (Learning Rate): I wanted the model to learn at a decent pace without overshooting. A lower value (e.g., 0.001) would make training too slow, while a higher value (e.g., 0.1) might make it unstable.\n",
    "- seed=42: Just to keep results reproducible. Classic choice.\n",
    "- batch_size=4: a small batch size helps capture more detailed patterns. I tried 68, 16, and 32. Larger batch sizes not working well. \n",
    "- epochs=1000: Autoencoders sometimes take a while to converge, so I wanted to give it enough time to optimize properly.\n",
    "- loss_function='binary_cross_entropy': Since we're dealing with classification problem, binary_cross_entropy makes sense as it works best for binary data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
